{
    "docs": [
        {
            "location": "/", 
            "text": "flume documentation\n\n\nflume\n (pronounced \nfloom\n) is a stream processing framework that can be\nused to create streaming pipelines with code in a reusable and dynamic manner.\nThese pipelines can read data from different types of sources through \nadapters\n\nand can write data out to \nadapters\n as well as visualizations through\n\nviews\n.", 
            "title": "Home"
        }, 
        {
            "location": "/#flume-documentation", 
            "text": "flume  (pronounced  floom ) is a stream processing framework that can be\nused to create streaming pipelines with code in a reusable and dynamic manner.\nThese pipelines can read data from different types of sources through  adapters \nand can write data out to  adapters  as well as visualizations through views .", 
            "title": "flume documentation"
        }, 
        {
            "location": "/overview/", 
            "text": "overview\n\n\nbasics\n\n\nA \nflume\n program is pipeline of \nflume\n nodes that read points from a \nsource\n\nand process those points using various \nflume\n procs (short for processors)\nwhich in turn push the points to a \nsink\n. Here's the basic structure of a\n\nflume\n program:\n\n\nsource | proc1 | ... | procN | sink\n\n\n\nNow the above is a very simplified view of the world since a \nflume\n program\ncan in fact consist of multiple sources, sinks and even split and rejoin\nstreams of data at various points in the pipeline. For example you can read\nfrom multiple sources and merge the points like so:\n\n\n(source1 ; source2 ; source3) | ... | sink\n\n\n\nThe parenthesis are used to expression parallel pipelines in \nflume\n and at\nthe end of the parenthesis block all those points are merged together in order\nunless you use one of set procs such as \nunion\n,\n\nintersect\n or \ndiff\n to create the new \nstream applying the specific set operation you want applied. This same \nability to handle \nN\n sources allows you to also split the stream at any point\nin your pipeline. So here's another somewhat less abstract example:\n\n\nsource\n| (\n    filter('value % 2 == 0') | put(even=count()),\n    filter('value % 2 != 0') | put(odd=count())\n) | sink\n\n\n\nThe above wouldn't be possible to calculate unless we split the stream since\nwe have to eliminate things from the stream on either side that are in direct\nconflict with the other type of calculation involved. In a more imperative \nway of doing this you'd also split the stream by keeping the intermediate value\nin two different variables tracking the odd vs even count.\n\n\nstreams and reducers\n\n\nFlume\n can handle streams of points with and without a valid time field. The\nmain reason you want a time field is calculate reductions over your stream of\npoints. Without a time field you can do things such as adding fields to your\ndata and/or joining with other streams to decorate those with missing\ninformation. Lets start by first diving into what a stream with a \ntime\n field\nlooks like:\n\n\nemit(limit=10) | write('stdio')\n\n\n\n\nThe previous \nflume\n program uses the \nemit\n proc to create\npoints that contain just a \ntime\n field starting as of right now and creating\na point every second until we've \"emitted\" a total of 10 points. The output\nof running the above with the \nflume\n CLI looks like so:\n\n\n flume \nemit(limit=10) | write('stdio')\n\n{\ntime\n: \n2016-07-30T15:18:15.562Z\n}\n{\ntime\n: \n2016-07-30T15:18:16.562Z\n}\n{\ntime\n: \n2016-07-30T15:18:17.562Z\n}\n{\ntime\n: \n2016-07-30T15:18:18.562Z\n}\n{\ntime\n: \n2016-07-30T15:18:19.562Z\n}\n{\ntime\n: \n2016-07-30T15:18:20.562Z\n}\n{\ntime\n: \n2016-07-30T15:18:21.562Z\n}\n{\ntime\n: \n2016-07-30T15:18:22.562Z\n}\n{\ntime\n: \n2016-07-30T15:18:23.562Z\n}\n{\ntime\n: \n2016-07-30T15:18:24.562Z\n}\n\n\n\n\nThe output format used is \njsonlines\n which allows for a\ntrue stream of data since each line represents a valid \nJSON\n object. The \nprevious stream has a \ntime\n field so we can used the \nreduce\n\nproc to calculate time aligned reductions. This is done by specifying the \nreduction we want to calculate and the new field we want to create with the\nresult of that reduction. Here's a simple example:\n\n\n flume \nemit(limit=10) | reduce(count=count()) | write('stdio')\n\n{\ncount\n: 10, \ntime\n: \n2016-07-30T15:22:30.033Z\n}\n\n\n\n\nNow the \nreducer\n used was \ncount\n\nwhich simply calculates for a given interval the number of points that passed\nthrough the \nreduce\n proc during that time. The interval itself\nis set with the argument \nevery\n to the \nreduce\n proc and in this case since\nwe didn't set it to anything we basically said to reduce over the full length\nof our stream.", 
            "title": "Overview"
        }, 
        {
            "location": "/overview/#overview", 
            "text": "", 
            "title": "overview"
        }, 
        {
            "location": "/overview/#basics", 
            "text": "A  flume  program is pipeline of  flume  nodes that read points from a  source \nand process those points using various  flume  procs (short for processors)\nwhich in turn push the points to a  sink . Here's the basic structure of a flume  program:  source | proc1 | ... | procN | sink  Now the above is a very simplified view of the world since a  flume  program\ncan in fact consist of multiple sources, sinks and even split and rejoin\nstreams of data at various points in the pipeline. For example you can read\nfrom multiple sources and merge the points like so:  (source1 ; source2 ; source3) | ... | sink  The parenthesis are used to expression parallel pipelines in  flume  and at\nthe end of the parenthesis block all those points are merged together in order\nunless you use one of set procs such as  union , intersect  or  diff  to create the new \nstream applying the specific set operation you want applied. This same \nability to handle  N  sources allows you to also split the stream at any point\nin your pipeline. So here's another somewhat less abstract example:  source\n| (\n    filter('value % 2 == 0') | put(even=count()),\n    filter('value % 2 != 0') | put(odd=count())\n) | sink  The above wouldn't be possible to calculate unless we split the stream since\nwe have to eliminate things from the stream on either side that are in direct\nconflict with the other type of calculation involved. In a more imperative \nway of doing this you'd also split the stream by keeping the intermediate value\nin two different variables tracking the odd vs even count.", 
            "title": "basics"
        }, 
        {
            "location": "/overview/#streams-and-reducers", 
            "text": "Flume  can handle streams of points with and without a valid time field. The\nmain reason you want a time field is calculate reductions over your stream of\npoints. Without a time field you can do things such as adding fields to your\ndata and/or joining with other streams to decorate those with missing\ninformation. Lets start by first diving into what a stream with a  time  field\nlooks like:  emit(limit=10) | write('stdio')  The previous  flume  program uses the  emit  proc to create\npoints that contain just a  time  field starting as of right now and creating\na point every second until we've \"emitted\" a total of 10 points. The output\nof running the above with the  flume  CLI looks like so:   flume  emit(limit=10) | write('stdio') \n{ time :  2016-07-30T15:18:15.562Z }\n{ time :  2016-07-30T15:18:16.562Z }\n{ time :  2016-07-30T15:18:17.562Z }\n{ time :  2016-07-30T15:18:18.562Z }\n{ time :  2016-07-30T15:18:19.562Z }\n{ time :  2016-07-30T15:18:20.562Z }\n{ time :  2016-07-30T15:18:21.562Z }\n{ time :  2016-07-30T15:18:22.562Z }\n{ time :  2016-07-30T15:18:23.562Z }\n{ time :  2016-07-30T15:18:24.562Z }  The output format used is  jsonlines  which allows for a\ntrue stream of data since each line represents a valid  JSON  object. The \nprevious stream has a  time  field so we can used the  reduce \nproc to calculate time aligned reductions. This is done by specifying the \nreduction we want to calculate and the new field we want to create with the\nresult of that reduction. Here's a simple example:   flume  emit(limit=10) | reduce(count=count()) | write('stdio') \n{ count : 10,  time :  2016-07-30T15:22:30.033Z }  Now the  reducer  used was  count \nwhich simply calculates for a given interval the number of points that passed\nthrough the  reduce  proc during that time. The interval itself\nis set with the argument  every  to the  reduce  proc and in this case since\nwe didn't set it to anything we basically said to reduce over the full length\nof our stream.", 
            "title": "streams and reducers"
        }, 
        {
            "location": "/installation/", 
            "text": "installation\n\n\nInstalling \nflume\n is as simple as any other \nPython\n package:\n\n\npip install flume\n\n\n\n\nAt this point you should have the \nflume\n command line utility which you can\nverify like so:\n\n\nflume \nemit(limit=3) | write('stdio')\n\n\n\n\n\nWhich should produce the following output over the course of 3s:\n\n\n{\ntime\n: \n2016-07-29T22:02:03.839Z\n}\n{\ntime\n: \n2016-07-29T22:02:04.839Z\n}\n{\ntime\n: \n2016-07-29T22:02:05.839Z\n}", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#installation", 
            "text": "Installing  flume  is as simple as any other  Python  package:  pip install flume  At this point you should have the  flume  command line utility which you can\nverify like so:  flume  emit(limit=3) | write('stdio')   Which should produce the following output over the course of 3s:  { time :  2016-07-29T22:02:03.839Z }\n{ time :  2016-07-29T22:02:04.839Z }\n{ time :  2016-07-29T22:02:05.839Z }", 
            "title": "installation"
        }, 
        {
            "location": "/first_steps/", 
            "text": "first steps\n\n\nFor most of the programs below we'll use the \nflume\n CLI since it allows us\nto quickly write and iterate on a program while immediately seeing the result\nof executing that program.\n\n\nyour first flume program\n\n\nLets start with writing a program where we generate some points using a special\nsource called \nemit\n which can emit points to simulate real\nworld data:\n\n\nflume \nemit(limit=10) | write('stdio')\n\n\n\n\n\nThe above emits 10 points to the sink \nwrite\n and pushing\nthose writes through the \nstdio\n adapter. The output will\nlook like so (each point is emitted in realtime):\n\n\n{\ntime\n: \n2016-07-17T19:55:40.853Z\n}\n{\ntime\n: \n2016-07-17T19:55:41.853Z\n}\n{\ntime\n: \n2016-07-17T19:55:42.853Z\n}\n{\ntime\n: \n2016-07-17T19:55:43.853Z\n}\n{\ntime\n: \n2016-07-17T19:55:44.853Z\n}\n{\ntime\n: \n2016-07-17T19:55:45.853Z\n}\n{\ntime\n: \n2016-07-17T19:55:46.853Z\n}\n{\ntime\n: \n2016-07-17T19:55:47.853Z\n}\n{\ntime\n: \n2016-07-17T19:55:48.853Z\n}\n{\ntime\n: \n2016-07-17T19:55:49.853Z\n}\n\n\n\n\nThose points \nemit\n in realtime one by one since the default\nvalue for the argument \nstart\n is set to \nnow\n, you can set \nstart\n to\nsomething such as \n2013-01-01\n and it would \nemit\n those\npoints immediately.\n\n\nreading from a log file\n\n\nNow let's do something more interesting and read actual data from a real source\nsuch as the syslog file in \nexamples/grok/syslog\n\n(download it from the source and change the path in the \nfile\n argument below,\nif you don't happen to not be working on a copy of the source of \nflume\n):\n\n\nflume \nread('stdio', format='grok', pattern='%{SYSLOGLINE}', file='examples/grok/syslog') | write('stdio')\n\n\n\n\n\nThe above uses quite a few things to achieve the desired result of parsing the\nsyslog file into data points in the \nflume\n stream. So there's the \nread\n\nprocessor which uses the \nstdio\n adapter to parse the file\n\nexamples/grok/syslog\n using the \ngrok\n\nstream parser. We don't have actual a \ntime\n field in our data so we should tell\nthe \nread\n processor which field in our data is a time field, like so:\n\n\nflume \nread('stdio', format='grok', pattern='%{SYSLOGLINE}', file='examples/grok/syslog', time='timestamp') | write('stdio')\n\n\n\n\n\nWe could now do something interesting to our data such as calculating how many\nlog lines we have per hour in this file. So we have the \nreduce\n processor\nwhich is used to calculate reductions on our stream. Nothing like an example\nto better show case how \nreduce\n is used:\n\n\nflume \nread('stdio', format='grok', pattern='%{SYSLOGLINE}', file='examples/grok/syslog', time='timestamp') | reduce(count=count(), every='1h') | write('stdio')\n\n\n\n\n\nOur command line is getting a little difficult to write on a single command\nline so we could use some feature to short hand certain parts of our pipeline.\nThis is all python code so really there already exists such shorthands by simply\ndefining new python functions that wrap existing \nflume\n procesors like so:\n\n\ndef syslog(filename):\n    return read('stdio', format='grok', pattern='%{SYSLOGLINE}', file=filename, time='timestamp')\n\n\n\n\nAnd to use your new syslog helper/alias you simply need to create a local\nfile with the name \n.flumerc.py\n which can contain utilities you can use\nwhen running the \nflume\n command line tool. The \n.flumerc.py\n file should look\nlike so:\n\n\nfrom flume import *\n\ndef syslog(file):\n    return read('stdio', format='grok', pattern='%{SYSLOGLINE}', file=file, time='timestamp')\n\n\n\n\nThe \n.flumerc.py\n file can be used to define anything you'd like to expose\nglobally for your \nflume\n command line programs. With the above \n.flumerc.py\n\nfile in your current working directory or globally accessible in your home (~/)\ndirectory you can now run the earlier program like so:\n\n\n flume \nsyslog('examples/grok/syslog') | reduce(count=count(), every='1h') | write('stdio')\n\n{\ncount\n: 27, \ntime\n: \n2016-07-17T13:59:44.000Z\n}\n{\ncount\n: 1, \ntime\n: \n2016-07-17T14:59:44.000Z\n}\n{\ncount\n: 110, \ntime\n: \n2016-07-17T15:59:44.000Z\n}\n{\ncount\n: 8, \ntime\n: \n2016-07-17T16:59:44.000Z\n}\n{\ncount\n: 118, \ntime\n: \n2016-07-17T17:59:44.000Z\n}\n{\ncount\n: 10, \ntime\n: \n2016-07-17T18:59:44.000Z\n}\n\n\n\n\nThat is a lot easier to read and write on the command line and also highlights\nthe main reason I wanted \nflume\n to be just an extension of the \npython\n\nruntime where you can simply use existing familiar constructs to build parts\nof the \nflume\n pipeline.\n\n\nNow what if we actually wanted to get the count of lines generated by each\nprogram writing to the syslog file. This can be easily achieved using the\nargument \nby\n to the \nreduce\n processor like so:\n\n\n flume \nsyslog('examples/grok/syslog') | reduce(count=count(), by=['program']) | write('stdio')\n\n{\ncount\n: 178, \nprogram\n: \nkernel\n, \ntime\n: \n2016-07-17T13:59:44.000Z\n}\n{\ncount\n: 21, \nprogram\n: \nlaptop-mode\n, \ntime\n: \n2016-07-17T13:59:44.000Z\n}\n{\ncount\n: 18, \nprogram\n: \nwpa_supplicant\n, \ntime\n: \n2016-07-17T13:59:44.000Z\n}\n{\ncount\n: 14, \nprogram\n: \nanacron\n, \ntime\n: \n2016-07-17T13:59:44.000Z\n}\n{\ncount\n: 19, \nprogram\n: \nCRON\n, \ntime\n: \n2016-07-17T13:59:44.000Z\n}\n{\ncount\n: 3, \nprogram\n: \ncinnamon-screensaver-dialog\n, \ntime\n: \n2016-07-17T13:59:44.000Z\n}\n{\ncount\n: 18, \nprogram\n: \nNetworkManager\n, \ntime\n: \n2016-07-17T13:59:44.000Z\n}\n{\ncount\n: 3, \nprogram\n: \nconsole-kit-daemon\n, \ntime\n: \n2016-07-17T13:59:44.000Z\n}\n\n\n\n\nMaking the above easier to read we could sort by \ncount\n using the \nsort\n\nprocessor and get something like so:\n\n\n flume \nsyslog('examples/grok/syslog') | reduce(count=count(), by=['program']) | sort('count') | write('stdio')\n\n{\ncount\n: 3, \nprogram\n: \ncinnamon-screensaver-dialog\n}\n{\ncount\n: 3, \nprogram\n: \nconsole-kit-daemon\n}\n{\ncount\n: 14, \nprogram\n: \nanacron\n}\n{\ncount\n: 18, \nprogram\n: \nwpa_supplicant\n}\n{\ncount\n: 18, \nprogram\n: \nNetworkManager\n}\n{\ncount\n: 19, \nprogram\n: \nCRON\n}\n{\ncount\n: 21, \nprogram\n: \nlaptop-mode\n}\n{\ncount\n: 178, \nprogram\n: \nkernel\n}\n\n\n\n\nWhich makes it easy to see that the \nkernel\n is responsible for the majority\nof log lines in our syslog file. For those wondering why the \ntime\n field just\ndisappeared from our output it's because we can't continue to do other things\ndownstream with points if they're not in chronological order.\n\n\nAt this point I'd bet some of you are saying well I can totally do all of the\nabove with my \nGNU\n command line tools. Of course you can and it would\nprobably look something like so:\n\n\n cat examples/grok/syslog | awk '{print $5}' | sed 's/\\[[0-9]*\\]//g' | sort | uniq -c | sort -n\n      3 cinnamon-screensaver-dialog:\n      3 console-kit-daemon:\n     14 anacron:\n     18 NetworkManager:\n     18 wpa_supplicant:\n     19 CRON:\n     21 laptop-mode:\n    178 kernel:\n\n\n\n\nIt is actually shorter than using \nflume\n but I doubt you'll find many people\nwho can read that in a single pass and understand what it does.\n\n\nreading from an HTTP request\n\n\nWe could have presented this earlier since with the \nhttp\n you\ncan read the syslog file mentioned in the previous section directly from github\nlike so:\n\n\nflume \nread('http', url='https://raw.githubusercontent.com/rlgomes/flume/master/examples/grok/syslog', format='grok', pattern='%{SYSLOGLINE}', time='timestamp') | write('stdio')\n\n\n\n\n\nThe above is getting a bit out of hand in terms of the length of the \nflume\n\nprogram so one could shorten it by creating a \n.flumerc.py\n helper like so:\n\n\ndef http_syslog(url):\n    return read('http', url=url, format='grok', pattern='%{SYSLOGLINE}', time='timestamp')\n\n\n\n\nWhich allows us to shorten the previous command line to simply:\n\n\nflume \nhttp_syslog('https://raw.githubusercontent.com/rlgomes/flume/master/examples/grok/syslog') | write('stdio')\n\n\n\n\n\nThis example is to really highlight how simple the transition is from reading\ndata from a local log file to reading an HTTP end point in order to get real\ndata into your pipeline.\n\n\nrunning flume in Python\n\n\nThe previous examples focused on showing off how to use \nflume\n from the\ncommand line but its ultimately intended to be used from your own Python source\ncode and so lets dive into how that works. \nFlume\n itself is just a python\nmodule that exports the various \nsources\n,\nsinks\n and \nprocs\n so you can use\nthem when building a \nflume\n pipeline. To stat you must import the elements\nyou want from flume like so:\n\n\nfrom flume import emit, count, put, write\n\n\n\n\nThen you can construct your flume pipeline as you did on the command line:\n\n\nfrom flume import emit, count, put, write\n\npipeline = emit(limit=5) | put(count=count()) | write('stdio')\n\n\n\n\nNow if you execute the above \nPython\n program you won't get any output and\nthat is because all you did above was construct a \nflume\n pipeline and it\nwon't execute till you tell it to do so using the \nexecute()\n method, like so:\n\n\nfrom flume import emit, count, put, write\n\npipeline = emit(limit=5) | put(count=count()) | write('stdio')\npipeline.execute()\n\n\n\n\nNow executing the above will produce in realtime, the following output:\n\n\n python test.py \n{\ncount\n: 1, \ntime\n: \n2016-07-30T10:40:50.068Z\n}\n{\ncount\n: 2, \ntime\n: \n2016-07-30T10:40:51.068Z\n}\n{\ncount\n: 3, \ntime\n: \n2016-07-30T10:40:52.068Z\n}\n{\ncount\n: 4, \ntime\n: \n2016-07-30T10:40:53.068Z\n}\n{\ncount\n: 5, \ntime\n: \n2016-07-30T10:40:54.068Z\n}\n\n\n\n\nThere we have our first \nflume\n pipeline in \nPython\n which can be rerun by\nsimple calling the \nexecute()\n method on the pipeline we constructed. The main\nreason for this type of construct is that it allows you to control the order\nof execution of multiple pipelines or even calculate a set of values in one\npipeline before you execute another pipeline that depends on the previous one.", 
            "title": "First Steps"
        }, 
        {
            "location": "/first_steps/#first-steps", 
            "text": "For most of the programs below we'll use the  flume  CLI since it allows us\nto quickly write and iterate on a program while immediately seeing the result\nof executing that program.", 
            "title": "first steps"
        }, 
        {
            "location": "/first_steps/#your-first-flume-program", 
            "text": "Lets start with writing a program where we generate some points using a special\nsource called  emit  which can emit points to simulate real\nworld data:  flume  emit(limit=10) | write('stdio')   The above emits 10 points to the sink  write  and pushing\nthose writes through the  stdio  adapter. The output will\nlook like so (each point is emitted in realtime):  { time :  2016-07-17T19:55:40.853Z }\n{ time :  2016-07-17T19:55:41.853Z }\n{ time :  2016-07-17T19:55:42.853Z }\n{ time :  2016-07-17T19:55:43.853Z }\n{ time :  2016-07-17T19:55:44.853Z }\n{ time :  2016-07-17T19:55:45.853Z }\n{ time :  2016-07-17T19:55:46.853Z }\n{ time :  2016-07-17T19:55:47.853Z }\n{ time :  2016-07-17T19:55:48.853Z }\n{ time :  2016-07-17T19:55:49.853Z }  Those points  emit  in realtime one by one since the default\nvalue for the argument  start  is set to  now , you can set  start  to\nsomething such as  2013-01-01  and it would  emit  those\npoints immediately.", 
            "title": "your first flume program"
        }, 
        {
            "location": "/first_steps/#reading-from-a-log-file", 
            "text": "Now let's do something more interesting and read actual data from a real source\nsuch as the syslog file in  examples/grok/syslog \n(download it from the source and change the path in the  file  argument below,\nif you don't happen to not be working on a copy of the source of  flume ):  flume  read('stdio', format='grok', pattern='%{SYSLOGLINE}', file='examples/grok/syslog') | write('stdio')   The above uses quite a few things to achieve the desired result of parsing the\nsyslog file into data points in the  flume  stream. So there's the  read \nprocessor which uses the  stdio  adapter to parse the file examples/grok/syslog  using the  grok \nstream parser. We don't have actual a  time  field in our data so we should tell\nthe  read  processor which field in our data is a time field, like so:  flume  read('stdio', format='grok', pattern='%{SYSLOGLINE}', file='examples/grok/syslog', time='timestamp') | write('stdio')   We could now do something interesting to our data such as calculating how many\nlog lines we have per hour in this file. So we have the  reduce  processor\nwhich is used to calculate reductions on our stream. Nothing like an example\nto better show case how  reduce  is used:  flume  read('stdio', format='grok', pattern='%{SYSLOGLINE}', file='examples/grok/syslog', time='timestamp') | reduce(count=count(), every='1h') | write('stdio')   Our command line is getting a little difficult to write on a single command\nline so we could use some feature to short hand certain parts of our pipeline.\nThis is all python code so really there already exists such shorthands by simply\ndefining new python functions that wrap existing  flume  procesors like so:  def syslog(filename):\n    return read('stdio', format='grok', pattern='%{SYSLOGLINE}', file=filename, time='timestamp')  And to use your new syslog helper/alias you simply need to create a local\nfile with the name  .flumerc.py  which can contain utilities you can use\nwhen running the  flume  command line tool. The  .flumerc.py  file should look\nlike so:  from flume import *\n\ndef syslog(file):\n    return read('stdio', format='grok', pattern='%{SYSLOGLINE}', file=file, time='timestamp')  The  .flumerc.py  file can be used to define anything you'd like to expose\nglobally for your  flume  command line programs. With the above  .flumerc.py \nfile in your current working directory or globally accessible in your home (~/)\ndirectory you can now run the earlier program like so:   flume  syslog('examples/grok/syslog') | reduce(count=count(), every='1h') | write('stdio') \n{ count : 27,  time :  2016-07-17T13:59:44.000Z }\n{ count : 1,  time :  2016-07-17T14:59:44.000Z }\n{ count : 110,  time :  2016-07-17T15:59:44.000Z }\n{ count : 8,  time :  2016-07-17T16:59:44.000Z }\n{ count : 118,  time :  2016-07-17T17:59:44.000Z }\n{ count : 10,  time :  2016-07-17T18:59:44.000Z }  That is a lot easier to read and write on the command line and also highlights\nthe main reason I wanted  flume  to be just an extension of the  python \nruntime where you can simply use existing familiar constructs to build parts\nof the  flume  pipeline.  Now what if we actually wanted to get the count of lines generated by each\nprogram writing to the syslog file. This can be easily achieved using the\nargument  by  to the  reduce  processor like so:   flume  syslog('examples/grok/syslog') | reduce(count=count(), by=['program']) | write('stdio') \n{ count : 178,  program :  kernel ,  time :  2016-07-17T13:59:44.000Z }\n{ count : 21,  program :  laptop-mode ,  time :  2016-07-17T13:59:44.000Z }\n{ count : 18,  program :  wpa_supplicant ,  time :  2016-07-17T13:59:44.000Z }\n{ count : 14,  program :  anacron ,  time :  2016-07-17T13:59:44.000Z }\n{ count : 19,  program :  CRON ,  time :  2016-07-17T13:59:44.000Z }\n{ count : 3,  program :  cinnamon-screensaver-dialog ,  time :  2016-07-17T13:59:44.000Z }\n{ count : 18,  program :  NetworkManager ,  time :  2016-07-17T13:59:44.000Z }\n{ count : 3,  program :  console-kit-daemon ,  time :  2016-07-17T13:59:44.000Z }  Making the above easier to read we could sort by  count  using the  sort \nprocessor and get something like so:   flume  syslog('examples/grok/syslog') | reduce(count=count(), by=['program']) | sort('count') | write('stdio') \n{ count : 3,  program :  cinnamon-screensaver-dialog }\n{ count : 3,  program :  console-kit-daemon }\n{ count : 14,  program :  anacron }\n{ count : 18,  program :  wpa_supplicant }\n{ count : 18,  program :  NetworkManager }\n{ count : 19,  program :  CRON }\n{ count : 21,  program :  laptop-mode }\n{ count : 178,  program :  kernel }  Which makes it easy to see that the  kernel  is responsible for the majority\nof log lines in our syslog file. For those wondering why the  time  field just\ndisappeared from our output it's because we can't continue to do other things\ndownstream with points if they're not in chronological order.  At this point I'd bet some of you are saying well I can totally do all of the\nabove with my  GNU  command line tools. Of course you can and it would\nprobably look something like so:   cat examples/grok/syslog | awk '{print $5}' | sed 's/\\[[0-9]*\\]//g' | sort | uniq -c | sort -n\n      3 cinnamon-screensaver-dialog:\n      3 console-kit-daemon:\n     14 anacron:\n     18 NetworkManager:\n     18 wpa_supplicant:\n     19 CRON:\n     21 laptop-mode:\n    178 kernel:  It is actually shorter than using  flume  but I doubt you'll find many people\nwho can read that in a single pass and understand what it does.", 
            "title": "reading from a log file"
        }, 
        {
            "location": "/first_steps/#reading-from-an-http-request", 
            "text": "We could have presented this earlier since with the  http  you\ncan read the syslog file mentioned in the previous section directly from github\nlike so:  flume  read('http', url='https://raw.githubusercontent.com/rlgomes/flume/master/examples/grok/syslog', format='grok', pattern='%{SYSLOGLINE}', time='timestamp') | write('stdio')   The above is getting a bit out of hand in terms of the length of the  flume \nprogram so one could shorten it by creating a  .flumerc.py  helper like so:  def http_syslog(url):\n    return read('http', url=url, format='grok', pattern='%{SYSLOGLINE}', time='timestamp')  Which allows us to shorten the previous command line to simply:  flume  http_syslog('https://raw.githubusercontent.com/rlgomes/flume/master/examples/grok/syslog') | write('stdio')   This example is to really highlight how simple the transition is from reading\ndata from a local log file to reading an HTTP end point in order to get real\ndata into your pipeline.", 
            "title": "reading from an HTTP request"
        }, 
        {
            "location": "/first_steps/#running-flume-in-python", 
            "text": "The previous examples focused on showing off how to use  flume  from the\ncommand line but its ultimately intended to be used from your own Python source\ncode and so lets dive into how that works.  Flume  itself is just a python\nmodule that exports the various  sources , sinks  and  procs  so you can use\nthem when building a  flume  pipeline. To stat you must import the elements\nyou want from flume like so:  from flume import emit, count, put, write  Then you can construct your flume pipeline as you did on the command line:  from flume import emit, count, put, write\n\npipeline = emit(limit=5) | put(count=count()) | write('stdio')  Now if you execute the above  Python  program you won't get any output and\nthat is because all you did above was construct a  flume  pipeline and it\nwon't execute till you tell it to do so using the  execute()  method, like so:  from flume import emit, count, put, write\n\npipeline = emit(limit=5) | put(count=count()) | write('stdio')\npipeline.execute()  Now executing the above will produce in realtime, the following output:   python test.py \n{ count : 1,  time :  2016-07-30T10:40:50.068Z }\n{ count : 2,  time :  2016-07-30T10:40:51.068Z }\n{ count : 3,  time :  2016-07-30T10:40:52.068Z }\n{ count : 4,  time :  2016-07-30T10:40:53.068Z }\n{ count : 5,  time :  2016-07-30T10:40:54.068Z }  There we have our first  flume  pipeline in  Python  which can be rerun by\nsimple calling the  execute()  method on the pipeline we constructed. The main\nreason for this type of construct is that it allows you to control the order\nof execution of multiple pipelines or even calculate a set of values in one\npipeline before you execute another pipeline that depends on the previous one.", 
            "title": "running flume in Python"
        }, 
        {
            "location": "/adapters/streamers/", 
            "text": "streamers\n\n\nStreamers are an abstraction on reading and writing different formats that\ncan be used by various \nadapters\n within \nflume\n. Not all\nadapters have a need for using streamers but for those that do it because\nquite easy to support various different formats.\n\n\nbuiltin\n\n\n\n\ncsv\n - Comma-Separated Values\n\n\ngrok\n - parse arbitrary text and structure it\n\n\njson\n - JavaScript Object Notation\n\n\njsonl\n - JSON Lines \n\n\n\n\nwrite your own\n\n\nAll streamers extend from the \nflume.adapters.streamres.base.Streamer\n class\nand implement the following methods:\n\n\n\n\n__init__\n with keyword arguments for toggling special features\n\n\nread(self, stream)\n read method that receives a \nPython\n an input\n                           stream and should generate the various \nflume\n \n                           points (\nfrom flume import Point\n) that were\n                           parsed from the stream.\n\n\nwrite(self, stream, points)\n write method that receives an output stream\n                                    and a list of points and should serialize\n                                    those points onto the stream.\n\n\neof(self, stream)\n optionally if you want to do something special when\n                          there are no more points to write then you can\n                          implement the eof method and write to the stream\n                          provided.\n\n\n\n\nOnce you've written your streamer you have to register it with \nflume\n using\nthe function \nregister_streamer(name, streamer)\n where the name is a unique\nstring identifying your adapter such as elastic, http, influxdb, etc. and the\nstreamer is the class implementation of your streamer.", 
            "title": "overview"
        }, 
        {
            "location": "/adapters/streamers/#streamers", 
            "text": "Streamers are an abstraction on reading and writing different formats that\ncan be used by various  adapters  within  flume . Not all\nadapters have a need for using streamers but for those that do it because\nquite easy to support various different formats.", 
            "title": "streamers"
        }, 
        {
            "location": "/adapters/streamers/#builtin", 
            "text": "csv  - Comma-Separated Values  grok  - parse arbitrary text and structure it  json  - JavaScript Object Notation  jsonl  - JSON Lines", 
            "title": "builtin"
        }, 
        {
            "location": "/adapters/streamers/#write-your-own", 
            "text": "All streamers extend from the  flume.adapters.streamres.base.Streamer  class\nand implement the following methods:   __init__  with keyword arguments for toggling special features  read(self, stream)  read method that receives a  Python  an input\n                           stream and should generate the various  flume  \n                           points ( from flume import Point ) that were\n                           parsed from the stream.  write(self, stream, points)  write method that receives an output stream\n                                    and a list of points and should serialize\n                                    those points onto the stream.  eof(self, stream)  optionally if you want to do something special when\n                          there are no more points to write then you can\n                          implement the eof method and write to the stream\n                          provided.   Once you've written your streamer you have to register it with  flume  using\nthe function  register_streamer(name, streamer)  where the name is a unique\nstring identifying your adapter such as elastic, http, influxdb, etc. and the\nstreamer is the class implementation of your streamer.", 
            "title": "write your own"
        }, 
        {
            "location": "/adapters/streamers/csv/", 
            "text": "csv\n\n\nThe \ncsv\n streamer can handle the majority of \ncsv\n files and even has a few\noptions to tweak the reading and writing of \ncsv\n data:\n\n\nread\n\n\nNo arguments exposed at this point for read operations.\n\n\nwrite\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nheaders\n\n\nboolean used to specify to print or not print the headers\n\n\nNo, default: \nTrue\n\n\n\n\n\n\ndelimiter\n\n\ndelimiter character to use when reading the data\n\n\nNo, defualt: \n,\n\n\n\n\n\n\nignore_whitespace\n\n\nboolean used to specify if any preceding/trailing whitespace should be ignored\n\n\nNo, default: \nFalse", 
            "title": "csv"
        }, 
        {
            "location": "/adapters/streamers/csv/#csv", 
            "text": "The  csv  streamer can handle the majority of  csv  files and even has a few\noptions to tweak the reading and writing of  csv  data:", 
            "title": "csv"
        }, 
        {
            "location": "/adapters/streamers/csv/#read", 
            "text": "No arguments exposed at this point for read operations.", 
            "title": "read"
        }, 
        {
            "location": "/adapters/streamers/csv/#write", 
            "text": "Argument  Description  Required?      headers  boolean used to specify to print or not print the headers  No, default:  True    delimiter  delimiter character to use when reading the data  No, defualt:  ,    ignore_whitespace  boolean used to specify if any preceding/trailing whitespace should be ignored  No, default:  False", 
            "title": "write"
        }, 
        {
            "location": "/adapters/streamers/grok/", 
            "text": "grok\n\n\nThe \ngrok\n streamer can handle parsing almost any unstructed lines of text by\nallowing you to define a \npattern\n on how the lines are constructed which is\nthen used to deconstruct those lines into its various fields. The arguments\nthat the \ngrok\n streamer currently exposes are:\n\n\nread\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nheaders\n\n\nboolean used to specify to print or not print the headers\n\n\nNo, default: \nTrue\n\n\n\n\n\n\n\n\nwrite\n\n\nGrok can not be used for writing data at this point in time.", 
            "title": "grok"
        }, 
        {
            "location": "/adapters/streamers/grok/#grok", 
            "text": "The  grok  streamer can handle parsing almost any unstructed lines of text by\nallowing you to define a  pattern  on how the lines are constructed which is\nthen used to deconstruct those lines into its various fields. The arguments\nthat the  grok  streamer currently exposes are:", 
            "title": "grok"
        }, 
        {
            "location": "/adapters/streamers/grok/#read", 
            "text": "Argument  Description  Required?      headers  boolean used to specify to print or not print the headers  No, default:  True", 
            "title": "read"
        }, 
        {
            "location": "/adapters/streamers/grok/#write", 
            "text": "Grok can not be used for writing data at this point in time.", 
            "title": "write"
        }, 
        {
            "location": "/adapters/streamers/json/", 
            "text": "json\n\n\nThe \nJSON\n streamer can be used to read or write any \nJSON\n data from an\nadapter that uses it. Here are the currently exposed arguments:\n\n\nread\n\n\nNo arguments exposed at this point for read operations.\n\n\nwrite\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\npretty\n\n\nboolean used to specify to pretty print the \nJSON\n output\n\n\nNo, default: \nFalse", 
            "title": "json"
        }, 
        {
            "location": "/adapters/streamers/json/#json", 
            "text": "The  JSON  streamer can be used to read or write any  JSON  data from an\nadapter that uses it. Here are the currently exposed arguments:", 
            "title": "json"
        }, 
        {
            "location": "/adapters/streamers/json/#read", 
            "text": "No arguments exposed at this point for read operations.", 
            "title": "read"
        }, 
        {
            "location": "/adapters/streamers/json/#write", 
            "text": "Argument  Description  Required?      pretty  boolean used to specify to pretty print the  JSON  output  No, default:  False", 
            "title": "write"
        }, 
        {
            "location": "/adapters/streamers/jsonl/", 
            "text": "jsonl\n\n\nThe \nJSONL\n streamer can be used to read or write any \nJSONL\n data from an\nadapter that uses it. Here are the currently exposed arguments:\n\n\nread\n\n\nNo arguments exposed at this point for read operations.\n\n\nwrite\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\npretty\n\n\nboolean used to specify to pretty print the \nJSON\n output\n\n\nNo, default: \nFalse", 
            "title": "jsonl"
        }, 
        {
            "location": "/adapters/streamers/jsonl/#jsonl", 
            "text": "The  JSONL  streamer can be used to read or write any  JSONL  data from an\nadapter that uses it. Here are the currently exposed arguments:", 
            "title": "jsonl"
        }, 
        {
            "location": "/adapters/streamers/jsonl/#read", 
            "text": "No arguments exposed at this point for read operations.", 
            "title": "read"
        }, 
        {
            "location": "/adapters/streamers/jsonl/#write", 
            "text": "Argument  Description  Required?      pretty  boolean used to specify to pretty print the  JSON  output  No, default:  False", 
            "title": "write"
        }, 
        {
            "location": "/adapters/", 
            "text": "adapters\n\n\nAdapters are the way you can get data in and out of the \nflume\n pipeline.\nThere are a set of built in ones that you can read about in the following \nsections and you can also register your own adapter which we'll dive into\nhow the current API for that works as well below.\n\n\nbuilt-in\n\n\n\n\nelastic\n\n\nhttp\n\n\nstdio\n\n\n\n\nwrite your own\n\n\nWriting your own adapter is encouraged as there's no way we'd ever have\nan adapter for every possible storage at this point in the development of\n\nflume\n. We also look forward to accepting any adapters written into the\ncore of \nflume\n as to better share with the community.\n\n\nTo write your own adapter you need to extend from the class\n\nflume.adapters.adapter.adapter\n and then implement the methods:\n\n\n\n\n\n\nname\n you must define the name of your adapter with a class level\n        attribute called name\n\n\n\n\n\n\n__init__(self, ...)\n where you initialize your adapter and accept all of\n        the keywords your read/write handlers are going to handle.\n\n\n\n\n\n\nread(self)\n: read method will handle reading points from your backend\n        that match the criteria identified by the arguments to your \n__init__\n\n        method and will yield those from the \nread\n method until there's\n        nothing left\n\n\n\n\n\n\nwrite(self, points)\n writes the points given to the underlying backend\n        and returns from this method once they've all been written out.\n\n\n\n\n\n\neof(self)\n called when there are no more points to write to the adapter\n        and you can proceed to close off any connections to your backend.\n\n\n\n\n\n\noptimize(self, child)\n this method is called before the \nread\n source\n        executes and allows the adapter to take a downstream operation such as\n        \nhead\n, \ntail\n, \nreduce\n and run it within the adapter's engine where the\n        data resides and by doing so optimize the \nflume\n program greatly since\n        we don't have to read all the data into \nflume\n and process it there.\n        There's a section below dedicated to explaining how to handle\n        optimizations within your own adapter. \n\n\n\n\n\n\nNow that you've written your adapter you can register it using the\n\nregister_streamer(cls)\n which will register your adapter with the name\nspecified by the class level attribute \nname\n.\n\n\noptimizations\n\n\nOptimizations are the way that \nflume\n takes a pipeline of operations and\nfigures out which ones can be executed on the backend (closer to where the data\nresides) so that you don't have to transport data from its source to where the\n\nflume\n pipeline is executing in order to compute something on that data or\nmake a decision about it. With that said the \noptimize\n method on the adapter\nallows you to see what is downstream from the adapter before the adapter starts\nrunning.\n\n\nAs an example the simplest thing to optimize in an adapter is when the following\nproc is the \nhead\n proc then you can simply set a limit\ninternally in your adapter and stop reading data once you've reached the desired\nfirst N values. This may not seem like much but can save a lot of time if you\nwere to only need the top 100 values but had 1000 of them to read from a backend\nand would simply stop much earlier pulling irrelevant points from the backend.\n\n\nHave a look at how we implemented the \noptimize\n method for\n\nelastic\n\nand \n\nstdio\n\nto get a simple view of how optimization can be done.", 
            "title": "overview"
        }, 
        {
            "location": "/adapters/#adapters", 
            "text": "Adapters are the way you can get data in and out of the  flume  pipeline.\nThere are a set of built in ones that you can read about in the following \nsections and you can also register your own adapter which we'll dive into\nhow the current API for that works as well below.", 
            "title": "adapters"
        }, 
        {
            "location": "/adapters/#built-in", 
            "text": "elastic  http  stdio", 
            "title": "built-in"
        }, 
        {
            "location": "/adapters/#write-your-own", 
            "text": "Writing your own adapter is encouraged as there's no way we'd ever have\nan adapter for every possible storage at this point in the development of flume . We also look forward to accepting any adapters written into the\ncore of  flume  as to better share with the community.  To write your own adapter you need to extend from the class flume.adapters.adapter.adapter  and then implement the methods:    name  you must define the name of your adapter with a class level\n        attribute called name    __init__(self, ...)  where you initialize your adapter and accept all of\n        the keywords your read/write handlers are going to handle.    read(self) : read method will handle reading points from your backend\n        that match the criteria identified by the arguments to your  __init__ \n        method and will yield those from the  read  method until there's\n        nothing left    write(self, points)  writes the points given to the underlying backend\n        and returns from this method once they've all been written out.    eof(self)  called when there are no more points to write to the adapter\n        and you can proceed to close off any connections to your backend.    optimize(self, child)  this method is called before the  read  source\n        executes and allows the adapter to take a downstream operation such as\n         head ,  tail ,  reduce  and run it within the adapter's engine where the\n        data resides and by doing so optimize the  flume  program greatly since\n        we don't have to read all the data into  flume  and process it there.\n        There's a section below dedicated to explaining how to handle\n        optimizations within your own adapter.     Now that you've written your adapter you can register it using the register_streamer(cls)  which will register your adapter with the name\nspecified by the class level attribute  name .", 
            "title": "write your own"
        }, 
        {
            "location": "/adapters/#optimizations", 
            "text": "Optimizations are the way that  flume  takes a pipeline of operations and\nfigures out which ones can be executed on the backend (closer to where the data\nresides) so that you don't have to transport data from its source to where the flume  pipeline is executing in order to compute something on that data or\nmake a decision about it. With that said the  optimize  method on the adapter\nallows you to see what is downstream from the adapter before the adapter starts\nrunning.  As an example the simplest thing to optimize in an adapter is when the following\nproc is the  head  proc then you can simply set a limit\ninternally in your adapter and stop reading data once you've reached the desired\nfirst N values. This may not seem like much but can save a lot of time if you\nwere to only need the top 100 values but had 1000 of them to read from a backend\nand would simply stop much earlier pulling irrelevant points from the backend.  Have a look at how we implemented the  optimize  method for elastic \nand  stdio \nto get a simple view of how optimization can be done.", 
            "title": "optimizations"
        }, 
        {
            "location": "/adapters/elastic/", 
            "text": "elastic adapter\n\n\nThe elastic adapter can be used to read and write data from a running\n\nelasticsearch\n instance. The currently supported versions of elasticsearch that we have integration tests to verify against is 2.3.3 . We'll be adding verification of a view other versions shortly and\nif you want to see support for a specific version please let us know on our\ngithub \nissues\n tracking system.\n\n\nread\n\n\nReading from the elastic adapter is done using the \nread\n source\nwith the following options available at this time:\n\n\nread('elastic',\n     index=None,\n     type='metric',\n     host='localhost',\n     port=9200,\n     time='time',\n     filter=None,\n     batch=1024) | ...\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nindex\n\n\nelasticsearch index to read from\n\n\nNo, default: \n_all\n for read, \nmetrics\n for write\n\n\n\n\n\n\ntype\n\n\nelasticsearch document type to read\n\n\nNo, default: \nmetric\n\n\n\n\n\n\nhost\n\n\nhostname of the elasticsearch instance\n\n\nNo, default: \nlocalhost\n\n\n\n\n\n\nport\n\n\nport of the elasticsearch instance\n\n\nNo, default: \n9200\n\n\n\n\n\n\ntime\n\n\nfield name that contains valid timestamp\n\n\nNo, default: \ntime\n\n\n\n\n\n\nfilter\n\n\nfilter expression to run against the es data (*)\n\n\nNo, default:  \nNone\n\n\n\n\n\n\nbatch\n\n\nthe read/write batch size\n\n\nNo, default: \n1024\n\n\n\n\n\n\n\n\n\n\n(*) filter expressions are further explained \nhere\n\n\n\n\nwrite\n\n\nWriting to the elastic adapter is done using the \nwrite\n sink\nwith the following options available at this time:\n\n\n... | write('elastic',\n            index='_all',\n            type='metric',\n            host='localhost',\n            port=9200,\n            time='time',\n            filter=None,\n            batch=1024)\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nindex\n\n\nelasticsearch index to write to\n\n\nNo, default: \n_all\n\n\n\n\n\n\ntype\n\n\nelasticsearch document type to write\n\n\nNo, default: \nmetric\n\n\n\n\n\n\nhost\n\n\nhostname of the elasticsearch instance\n\n\nNo, default: \nlocalhost\n\n\n\n\n\n\nport\n\n\nport of the elasticsearch instance\n\n\nNo, default: \n9200\n\n\n\n\n\n\n\n\nwriting a few points to elastic\n\n\nfrom flume import emit, write\n\n(\n    emit(limit=10, start='2013-01-01', every='day')\n    | write('elastic', index='test-index')\n).execute()\n\n\n\n\nTo test out the above using a quick elasticsearch instance spun up using docker\njust use the following quick command line:\n\n\ndocker run -p 9200:9200 elasticsearch:2.3.3\n\n\n\n\nThen you can execute the program, but there will be no output and you can easily\nverify your data was stored to that local instance by hitting:\n\n\nhttp://localhost:9200/test-index/_search\n\n\n\nWhere the JSON response should have 10 hits and you should be able to see the\n\n_source\n field contains the timestamps from \n2013-01-01T00:00:00.000Z\n to \n\n2013-01-101T00:00:00.000Z\n (they're not sorted at this point).\n\n\nreading points from elastic\n\n\nfrom flume import read, write\n\n(\n    read('elastic', index='test-index')\n    | write('stdio')\n).execute()\n\n\n\n\nWhich produces the following output:\n\n\n python test.py \n{\ntime\n: \n2013-01-01T00:00:00.000Z\n}\n{\ntime\n: \n2013-01-02T00:00:00.000Z\n}\n{\ntime\n: \n2013-01-03T00:00:00.000Z\n}\n{\ntime\n: \n2013-01-04T00:00:00.000Z\n}\n{\ntime\n: \n2013-01-05T00:00:00.000Z\n}\n{\ntime\n: \n2013-01-06T00:00:00.000Z\n}\n{\ntime\n: \n2013-01-07T00:00:00.000Z\n}\n{\ntime\n: \n2013-01-08T00:00:00.000Z\n}\n{\ntime\n: \n2013-01-09T00:00:00.000Z\n}\n{\ntime\n: \n2013-01-10T00:00:00.000Z\n}", 
            "title": "elastic"
        }, 
        {
            "location": "/adapters/elastic/#elastic-adapter", 
            "text": "The elastic adapter can be used to read and write data from a running elasticsearch  instance. The currently supported versions of elasticsearch that we have integration tests to verify against is 2.3.3 . We'll be adding verification of a view other versions shortly and\nif you want to see support for a specific version please let us know on our\ngithub  issues  tracking system.", 
            "title": "elastic adapter"
        }, 
        {
            "location": "/adapters/elastic/#read", 
            "text": "Reading from the elastic adapter is done using the  read  source\nwith the following options available at this time:  read('elastic',\n     index=None,\n     type='metric',\n     host='localhost',\n     port=9200,\n     time='time',\n     filter=None,\n     batch=1024) | ...     Argument  Description  Required?      index  elasticsearch index to read from  No, default:  _all  for read,  metrics  for write    type  elasticsearch document type to read  No, default:  metric    host  hostname of the elasticsearch instance  No, default:  localhost    port  port of the elasticsearch instance  No, default:  9200    time  field name that contains valid timestamp  No, default:  time    filter  filter expression to run against the es data (*)  No, default:   None    batch  the read/write batch size  No, default:  1024      (*) filter expressions are further explained  here", 
            "title": "read"
        }, 
        {
            "location": "/adapters/elastic/#write", 
            "text": "Writing to the elastic adapter is done using the  write  sink\nwith the following options available at this time:  ... | write('elastic',\n            index='_all',\n            type='metric',\n            host='localhost',\n            port=9200,\n            time='time',\n            filter=None,\n            batch=1024)     Argument  Description  Required?      index  elasticsearch index to write to  No, default:  _all    type  elasticsearch document type to write  No, default:  metric    host  hostname of the elasticsearch instance  No, default:  localhost    port  port of the elasticsearch instance  No, default:  9200", 
            "title": "write"
        }, 
        {
            "location": "/adapters/elastic/#writing-a-few-points-to-elastic", 
            "text": "from flume import emit, write\n\n(\n    emit(limit=10, start='2013-01-01', every='day')\n    | write('elastic', index='test-index')\n).execute()  To test out the above using a quick elasticsearch instance spun up using docker\njust use the following quick command line:  docker run -p 9200:9200 elasticsearch:2.3.3  Then you can execute the program, but there will be no output and you can easily\nverify your data was stored to that local instance by hitting:  http://localhost:9200/test-index/_search  Where the JSON response should have 10 hits and you should be able to see the _source  field contains the timestamps from  2013-01-01T00:00:00.000Z  to  2013-01-101T00:00:00.000Z  (they're not sorted at this point).", 
            "title": "writing a few points to elastic"
        }, 
        {
            "location": "/adapters/elastic/#reading-points-from-elastic", 
            "text": "from flume import read, write\n\n(\n    read('elastic', index='test-index')\n    | write('stdio')\n).execute()  Which produces the following output:   python test.py \n{ time :  2013-01-01T00:00:00.000Z }\n{ time :  2013-01-02T00:00:00.000Z }\n{ time :  2013-01-03T00:00:00.000Z }\n{ time :  2013-01-04T00:00:00.000Z }\n{ time :  2013-01-05T00:00:00.000Z }\n{ time :  2013-01-06T00:00:00.000Z }\n{ time :  2013-01-07T00:00:00.000Z }\n{ time :  2013-01-08T00:00:00.000Z }\n{ time :  2013-01-09T00:00:00.000Z }\n{ time :  2013-01-10T00:00:00.000Z }", 
            "title": "reading points from elastic"
        }, 
        {
            "location": "/adapters/http/", 
            "text": "http adapter\n\n\nThe \nhttp\n adapter can be used to read and write data to and from an HTTP server.\n\n\nread\n\n\nReading from an HTTP request is done using the \nread\n source\nwith the following options available at this time:\n\n\nread('http',\n     url=None,\n     method='GET',\n     headrs=None,\n     time='time',\n     follow_link=True,\n     format=None,\n     cache=None,\n     status=200) | ...\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nurl\n\n\nURL to hit when issuing the HTTP request\n\n\nYes\n\n\n\n\n\n\nmethod\n\n\nmethod to use when issuing the HTTP request\n\n\nNo, default: \nGET\n\n\n\n\n\n\nheaders\n\n\nheaders used when issuing the HTTP request\n\n\nNo, default: \nNone\n\n\n\n\n\n\ntime\n\n\nfield name that contains valid timestamp\n\n\nNo, default: \ntime\n\n\n\n\n\n\nfollow_link\n\n\nboolean that indicates to follow the HTTP Link header (*)\n\n\nNo, default: \nTrue\n\n\n\n\n\n\nformat\n\n\nformat specifier used to pick a different kind of \nstreamer\n\n\nNo, default: \nNone\n\n\n\n\n\n\ncache\n\n\nspecify a cache file to use for HTTP caching with \nrequests-cache\n (**)\n\n\nNo, default: \nNone\n\n\n\n\n\n\nstatus\n\n\nspecify the accepted status or statuses (list) for a successful request\n\n\nNo, default: \n200\n\n\n\n\n\n\n\n\n\n\n(*) \nLink header RFC\n\n\n(**) \nrequests-cache documentation\n\n\n\n\nwrite\n\n\nWriting to the HTTP adapter is done using the \nwrite\n sink with\nthe following options available at this time:\n\n\n... | write('http',\n            url=None,\n            method='GET',\n            headrs=None,\n            array=True,\n            status=200)\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nurl\n\n\nURL to hit when issuing the HTTP request\n\n\nYes\n\n\n\n\n\n\nmethod\n\n\nmethod to use when issuing the HTTP request\n\n\nNo, default: \nGET\n\n\n\n\n\n\nheaders\n\n\nheaders used when issuing the HTTP request\n\n\nNo, default: \nNone\n\n\n\n\n\n\narray\n\n\nspecify if the payload of the HTTP request should be an array or JSON object\n\n\nNo, default: \nFalse\n\n\n\n\n\n\nstatus\n\n\nspecify the accepted status or statuses (list) for a successful request\n\n\nNo, default: \n200\n\n\n\n\n\n\n\n\nreading some points from Github\n\n\nfrom flume import count, read, reduce, write\n\n(\n    read('http',\n         url='https://api.github.com/repos/rlgomes/flume/commits',\n         cache='flume-http-cache',\n         time='commit.author.date')\n    | reduce(count=count())\n    | write('stdio')\n).execute()\n\n\n\n\nThe above \nflume\n program calculates how many commits there have been to the\n\nflume\n github repo since it was first created. I turned the cache on so you\nwouldn't hit any Github Rate Limiting issues and if you want to get the latest\nand greatest data just delete the \nflume-http-cache.sqlite\n file.\n\n\nwriting some points\n\n\nfrom flume import *\n\noauth_token = raw_input('Provide your github oauth token: ')\n\n(\n    emit(limit=1)\n    | put(description='gist generated from flume at {time}',\n          public=True,\n          files={\n              'example.txt': {\n                  'content': 'just a silly example of what you can do'\n              }\n          })\n    | remove('time')\n    | write('http',\n            url='https://api.github.com/gists',\n            method='POST',\n            headers={\n                'authorization': 'token %s' % oauth_token,\n                'content-type': 'application/json'\n            },\n            array=False)\n).execute()\n\n\n\n\nIn order to run the above you'll have to create a throw away oauth token\nat https://github.com/settings/tokens and remember to scroll down and give\naccess to create gists. Then when you run it you won't get any output but \nif you visit https://gist.github.com/ the last gist created should be called\n\nexample.txt\n and you should see all the details we created above.", 
            "title": "http"
        }, 
        {
            "location": "/adapters/http/#http-adapter", 
            "text": "The  http  adapter can be used to read and write data to and from an HTTP server.", 
            "title": "http adapter"
        }, 
        {
            "location": "/adapters/http/#read", 
            "text": "Reading from an HTTP request is done using the  read  source\nwith the following options available at this time:  read('http',\n     url=None,\n     method='GET',\n     headrs=None,\n     time='time',\n     follow_link=True,\n     format=None,\n     cache=None,\n     status=200) | ...     Argument  Description  Required?      url  URL to hit when issuing the HTTP request  Yes    method  method to use when issuing the HTTP request  No, default:  GET    headers  headers used when issuing the HTTP request  No, default:  None    time  field name that contains valid timestamp  No, default:  time    follow_link  boolean that indicates to follow the HTTP Link header (*)  No, default:  True    format  format specifier used to pick a different kind of  streamer  No, default:  None    cache  specify a cache file to use for HTTP caching with  requests-cache  (**)  No, default:  None    status  specify the accepted status or statuses (list) for a successful request  No, default:  200      (*)  Link header RFC  (**)  requests-cache documentation", 
            "title": "read"
        }, 
        {
            "location": "/adapters/http/#write", 
            "text": "Writing to the HTTP adapter is done using the  write  sink with\nthe following options available at this time:  ... | write('http',\n            url=None,\n            method='GET',\n            headrs=None,\n            array=True,\n            status=200)     Argument  Description  Required?      url  URL to hit when issuing the HTTP request  Yes    method  method to use when issuing the HTTP request  No, default:  GET    headers  headers used when issuing the HTTP request  No, default:  None    array  specify if the payload of the HTTP request should be an array or JSON object  No, default:  False    status  specify the accepted status or statuses (list) for a successful request  No, default:  200", 
            "title": "write"
        }, 
        {
            "location": "/adapters/http/#reading-some-points-from-github", 
            "text": "from flume import count, read, reduce, write\n\n(\n    read('http',\n         url='https://api.github.com/repos/rlgomes/flume/commits',\n         cache='flume-http-cache',\n         time='commit.author.date')\n    | reduce(count=count())\n    | write('stdio')\n).execute()  The above  flume  program calculates how many commits there have been to the flume  github repo since it was first created. I turned the cache on so you\nwouldn't hit any Github Rate Limiting issues and if you want to get the latest\nand greatest data just delete the  flume-http-cache.sqlite  file.", 
            "title": "reading some points from Github"
        }, 
        {
            "location": "/adapters/http/#writing-some-points", 
            "text": "from flume import *\n\noauth_token = raw_input('Provide your github oauth token: ')\n\n(\n    emit(limit=1)\n    | put(description='gist generated from flume at {time}',\n          public=True,\n          files={\n              'example.txt': {\n                  'content': 'just a silly example of what you can do'\n              }\n          })\n    | remove('time')\n    | write('http',\n            url='https://api.github.com/gists',\n            method='POST',\n            headers={\n                'authorization': 'token %s' % oauth_token,\n                'content-type': 'application/json'\n            },\n            array=False)\n).execute()  In order to run the above you'll have to create a throw away oauth token\nat https://github.com/settings/tokens and remember to scroll down and give\naccess to create gists. Then when you run it you won't get any output but \nif you visit https://gist.github.com/ the last gist created should be called example.txt  and you should see all the details we created above.", 
            "title": "writing some points"
        }, 
        {
            "location": "/adapters/stdio/", 
            "text": "stdio adapter\n\n\nThe \nstdio\n adapter can be used to read and write to and from any file input.\nThis includes the stdin/stdout of a process but can also mean reading/writing\nfrom a file on the file system.\n\n\nread\n\n\nReading from \nstdio\n is done using the \nread\n source with the\nfollowing options available at this time:\n\n\nread('stdio',\n     format='jsonl',\n     file=None,\n     strip_ansi=False,\n     time='time',\n     compression=None) | ...\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nformat\n\n\nformat specifier used to pick a different kind of \nstreamer\n\n\nNo, default: \njsonl\n\n\n\n\n\n\nfile\n\n\nfilename to read from, when not specified we read from STDIN\n\n\nNo, default: \nNone\n\n\n\n\n\n\nstrip_ansi\n\n\nwhen set to \nTrue\n then all ANSI sequences are removed from the input\n\n\nNo, default: \nFalse\n\n\n\n\n\n\ntime\n\n\nfield name that contains valid timestamp\n\n\nNo, default: \ntime\n\n\n\n\n\n\ncompression\n\n\nspecify the compression algorithm to use (*)\n\n\nNo, default: \nNone\n\n\n\n\n\n\n\n\n(*) supported compression formats: gzip, zlib, deflate\n\n\nwrite\n\n\nWriting to \nstdio\n is done using the \nwrite\n sink with the\nfollowing options available at this time:\n\n\nwrite('stdio',\n      format='jsonl',\n      file=None,\n      append=False,\n      time='time',\n      compression=None) | ...\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nformat\n\n\nformat specifier used to pick a different kind of \nstreamer\n\n\nNo, default: \njsonl\n\n\n\n\n\n\nfile\n\n\nfilename to write to, when not specified we read from STDOUT\n\n\nNo, default: \nNone\n\n\n\n\n\n\nappend\n\n\nboolean that specifies if we should append or not to any existing output\n\n\nNo, default: \nFalse\n\n\n\n\n\n\ntime\n\n\nfield name that contains valid timestamp\n\n\nNo, default: \ntime\n\n\n\n\n\n\ncompression\n\n\nspecify the compression algorithm to use (*)\n\n\nNo, default: \nNone\n\n\n\n\n\n\n\n\n(*) supported compression formats: gzip, zlib, deflate\n\n\nreading data from dmesg\n\n\nThe \ndmesg\n command has output that looks like so:\n\n\n[83281.308572] EXT4-fs (dm-1): re-mounted. Opts: errors=remount-ro,commit=0\n[83281.335411] EXT4-fs (sda2): re-mounted. Opts: (null)\n[83281.417356] device vboxnet4 entered promiscuous mode\n[83281.524426] [drm] RC6 on\n[83284.403642] wlan0: authenticate with 10:05:b1:e0:83:20\n[83284.414489] wlan0: send auth to 10:05:b1:e0:83:20 (try 1/3)\n[83284.420835] wlan0: authenticated\n[83284.424327] wlan0: associate with 10:05:b1:e0:83:20 (try 1/3)\n[83284.427721] wlan0: RX AssocResp from 10:05:b1:e0:83:20 (capab=0x411 status=0 aid=6)\n[83284.430732] wlan0: associated\n[83285.900045] psmouse serio2: trackpoint: IBM TrackPoint firmware: 0x0e, buttons: 3/3\n\n\n\n\nLuckily we have built in support for parsing data with \ngrok\n\nwhich allows us to parse the above using a pattern like so:\n\n\n\\[\\s+%{BASE10NUM:timestamp}\\]\\s+%{WORD:app}%{GREEDYDATA:message}')\n\n\n\nWe're going with a simple approach to getting the timestamp as a number and\nalso the first thing that is not a space becomes the name of the application\nthat logged the line to the dmesg log. Now with the above expression we can use\nthe \nstdio\n adapter to figure out which are the top 5 applications that have\nbeen writing to dmesg:\n\n\nfrom flume import *\n\n(\n    read('stdio',\n         format='grok',\n         pattern='\\[\\s*%{BASE10NUM:timestamp}\\]\\s+%{NOTSPACE:app}%{GREEDYDATA:message}')\n    | reduce(count=count(), by=['app'])\n    | sort('count', order='desc')\n    | head(5)\n    | write('stdio')\n).execute()\n\n\n\n\nThe above is a but crude but gives us some interesting output which shows on my\nlaptop that the top 5 spammers include:\n\n\n dmesg  | python test.py\n{\ncount\n: 361, \napp\n: \nPM:\n}\n{\ncount\n: 320, \napp\n: \nwlan0:\n}\n{\ncount\n: 219, \napp\n: \nata2.00:\n}\n{\ncount\n: 200, \napp\n: \nusb\n}\n{\ncount\n: 180, \napp\n: \nsmpboot:\n}\n\n\n\n\nWhich means my wifi adapter and the power management services spam the dmesg log\nthe most. The previous execution also shows how the \nstdio\n adapter handles\nreading data directly from the \nSTDIN\n so you can pipe data from other commands\ndirectly to the \nPython\n process.\n\n\nwriting CSV data\n\n\nfrom flume import *\n\n(\n    read('stdio',\n         format='grok',\n         pattern='\\[\\s*%{BASE10NUM:timestamp}\\]\\s+%{NOTSPACE:app}%{GREEDYDATA:message}')\n    | write('stdio', format='csv', file='dmesg.csv')\n).execute()\n\n\n\n\nThe above will transform the \ndmesg\n input provided to this \nPython\n program\nthrough \nSTDIN\n will be written out in a \nCSV\n format to the \ndmesg.csv\n file.\nThe \nCSV\n file will contain the columns timestamp, app and message and can be\neasily imported into your favorite spreadsheet application", 
            "title": "stdio"
        }, 
        {
            "location": "/adapters/stdio/#stdio-adapter", 
            "text": "The  stdio  adapter can be used to read and write to and from any file input.\nThis includes the stdin/stdout of a process but can also mean reading/writing\nfrom a file on the file system.", 
            "title": "stdio adapter"
        }, 
        {
            "location": "/adapters/stdio/#read", 
            "text": "Reading from  stdio  is done using the  read  source with the\nfollowing options available at this time:  read('stdio',\n     format='jsonl',\n     file=None,\n     strip_ansi=False,\n     time='time',\n     compression=None) | ...     Argument  Description  Required?      format  format specifier used to pick a different kind of  streamer  No, default:  jsonl    file  filename to read from, when not specified we read from STDIN  No, default:  None    strip_ansi  when set to  True  then all ANSI sequences are removed from the input  No, default:  False    time  field name that contains valid timestamp  No, default:  time    compression  specify the compression algorithm to use (*)  No, default:  None     (*) supported compression formats: gzip, zlib, deflate", 
            "title": "read"
        }, 
        {
            "location": "/adapters/stdio/#write", 
            "text": "Writing to  stdio  is done using the  write  sink with the\nfollowing options available at this time:  write('stdio',\n      format='jsonl',\n      file=None,\n      append=False,\n      time='time',\n      compression=None) | ...     Argument  Description  Required?      format  format specifier used to pick a different kind of  streamer  No, default:  jsonl    file  filename to write to, when not specified we read from STDOUT  No, default:  None    append  boolean that specifies if we should append or not to any existing output  No, default:  False    time  field name that contains valid timestamp  No, default:  time    compression  specify the compression algorithm to use (*)  No, default:  None     (*) supported compression formats: gzip, zlib, deflate", 
            "title": "write"
        }, 
        {
            "location": "/adapters/stdio/#reading-data-from-dmesg", 
            "text": "The  dmesg  command has output that looks like so:  [83281.308572] EXT4-fs (dm-1): re-mounted. Opts: errors=remount-ro,commit=0\n[83281.335411] EXT4-fs (sda2): re-mounted. Opts: (null)\n[83281.417356] device vboxnet4 entered promiscuous mode\n[83281.524426] [drm] RC6 on\n[83284.403642] wlan0: authenticate with 10:05:b1:e0:83:20\n[83284.414489] wlan0: send auth to 10:05:b1:e0:83:20 (try 1/3)\n[83284.420835] wlan0: authenticated\n[83284.424327] wlan0: associate with 10:05:b1:e0:83:20 (try 1/3)\n[83284.427721] wlan0: RX AssocResp from 10:05:b1:e0:83:20 (capab=0x411 status=0 aid=6)\n[83284.430732] wlan0: associated\n[83285.900045] psmouse serio2: trackpoint: IBM TrackPoint firmware: 0x0e, buttons: 3/3  Luckily we have built in support for parsing data with  grok \nwhich allows us to parse the above using a pattern like so:  \\[\\s+%{BASE10NUM:timestamp}\\]\\s+%{WORD:app}%{GREEDYDATA:message}')  We're going with a simple approach to getting the timestamp as a number and\nalso the first thing that is not a space becomes the name of the application\nthat logged the line to the dmesg log. Now with the above expression we can use\nthe  stdio  adapter to figure out which are the top 5 applications that have\nbeen writing to dmesg:  from flume import *\n\n(\n    read('stdio',\n         format='grok',\n         pattern='\\[\\s*%{BASE10NUM:timestamp}\\]\\s+%{NOTSPACE:app}%{GREEDYDATA:message}')\n    | reduce(count=count(), by=['app'])\n    | sort('count', order='desc')\n    | head(5)\n    | write('stdio')\n).execute()  The above is a but crude but gives us some interesting output which shows on my\nlaptop that the top 5 spammers include:   dmesg  | python test.py\n{ count : 361,  app :  PM: }\n{ count : 320,  app :  wlan0: }\n{ count : 219,  app :  ata2.00: }\n{ count : 200,  app :  usb }\n{ count : 180,  app :  smpboot: }  Which means my wifi adapter and the power management services spam the dmesg log\nthe most. The previous execution also shows how the  stdio  adapter handles\nreading data directly from the  STDIN  so you can pipe data from other commands\ndirectly to the  Python  process.", 
            "title": "reading data from dmesg"
        }, 
        {
            "location": "/adapters/stdio/#writing-csv-data", 
            "text": "from flume import *\n\n(\n    read('stdio',\n         format='grok',\n         pattern='\\[\\s*%{BASE10NUM:timestamp}\\]\\s+%{NOTSPACE:app}%{GREEDYDATA:message}')\n    | write('stdio', format='csv', file='dmesg.csv')\n).execute()  The above will transform the  dmesg  input provided to this  Python  program\nthrough  STDIN  will be written out in a  CSV  format to the  dmesg.csv  file.\nThe  CSV  file will contain the columns timestamp, app and message and can be\neasily imported into your favorite spreadsheet application", 
            "title": "writing CSV data"
        }, 
        {
            "location": "/moments/", 
            "text": "moments\n\n\nA moment (\nfrom flume import moment\n) is how you can refer to a specific date\nin time or a duration using a human friendly expressions. Further details on\nhow to express each one of these in the following sections.\n\n\ndate\n\n\nA moment date is a human friendly expression that defines a moment in time. For\nexample if you wanted to refer to an exact date in time you would simply use\nthe \nISO8601\n format, like any of the\nfollowing examples:\n\n\n'2001-01-01T00:00:00.000Z'\n'1998-07-03T02:00:00.000Z'\n'2001-03-02'\n\n\n\nYou can also express dates in the following formats:\n\n\n'Sat Oct 11 17:13:46 UTC 2003'\n'next Saturday'\n'last Friday at 2PM'\n'one hour ago'\n\n\n\nWith the above you can see how its useful to be able to express dates in a more\nhuman friendly manner. All dates are converted to a \nPython\n\n\ndatetime\n object.\n\n\nduration\n\n\nDurations are also useful to express in a more human friendly format since\nbeing able to express durations like so:\n\n\n'2 seocnds'\n'45 minutes'\n'1 hour 30 minutes'\n'3 months'\n\n\n\nAnd knowing that \nflume\n will figure out that the \n3 months\n expression\nshould be calculated taking into account the exact moment in time that the\nstream is processing and calculating the exact distance for those 3 months\nto pass. To better understand if you calculate \n3 months\n from January the 1st\non a non leap year then you would end up with a duration of 31 + 28 + 31 days,\nbut on a leap year you'd end up with 31 + 29 + 31 days.  All durations are\nconverted to a \ntimedelta\n\nobject.", 
            "title": "Moments"
        }, 
        {
            "location": "/moments/#moments", 
            "text": "A moment ( from flume import moment ) is how you can refer to a specific date\nin time or a duration using a human friendly expressions. Further details on\nhow to express each one of these in the following sections.", 
            "title": "moments"
        }, 
        {
            "location": "/moments/#date", 
            "text": "A moment date is a human friendly expression that defines a moment in time. For\nexample if you wanted to refer to an exact date in time you would simply use\nthe  ISO8601  format, like any of the\nfollowing examples:  '2001-01-01T00:00:00.000Z'\n'1998-07-03T02:00:00.000Z'\n'2001-03-02'  You can also express dates in the following formats:  'Sat Oct 11 17:13:46 UTC 2003'\n'next Saturday'\n'last Friday at 2PM'\n'one hour ago'  With the above you can see how its useful to be able to express dates in a more\nhuman friendly manner. All dates are converted to a  Python  datetime  object.", 
            "title": "date"
        }, 
        {
            "location": "/moments/#duration", 
            "text": "Durations are also useful to express in a more human friendly format since\nbeing able to express durations like so:  '2 seocnds'\n'45 minutes'\n'1 hour 30 minutes'\n'3 months'  And knowing that  flume  will figure out that the  3 months  expression\nshould be calculated taking into account the exact moment in time that the\nstream is processing and calculating the exact distance for those 3 months\nto pass. To better understand if you calculate  3 months  from January the 1st\non a non leap year then you would end up with a duration of 31 + 28 + 31 days,\nbut on a leap year you'd end up with 31 + 29 + 31 days.  All durations are\nconverted to a  timedelta \nobject.", 
            "title": "duration"
        }, 
        {
            "location": "/procs/", 
            "text": "procs\n\n\nProcs (or processors) can do everything from augmenting the points in a stream\nto remove them completely from said stream. There are quite a few built in ones\nand you can always create your own. \n\n\nbuilt-in\n\n\n\n\ndiff\n\n\nfilter\n\n\nhead\n\n\nintersect\n\n\nkeep\n\n\nput\n\n\nremove\n\n\nseq\n\n\nsort\n\n\ntail\n\n\nunion\n\n\n\n\nwrite your own\n\n\n... coming soon ...", 
            "title": "overview"
        }, 
        {
            "location": "/procs/#procs", 
            "text": "Procs (or processors) can do everything from augmenting the points in a stream\nto remove them completely from said stream. There are quite a few built in ones\nand you can always create your own.", 
            "title": "procs"
        }, 
        {
            "location": "/procs/#built-in", 
            "text": "diff  filter  head  intersect  keep  put  remove  seq  sort  tail  union", 
            "title": "built-in"
        }, 
        {
            "location": "/procs/#write-your-own", 
            "text": "... coming soon ...", 
            "title": "write your own"
        }, 
        {
            "location": "/procs/diff/", 
            "text": "diff\n\n\nThe \ndiff\n proc is part of the set operators that can be used to join two\nstreams back into one but applying some set operator to the items of the\nstream. In this case only allow points that don't appear in all of the\ninputs.\n\n\n( ... , ....) | diff(*fieldnames) | ...\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\n*fieldnames\n\n\nlist or tuple of field names to calculate the diff on\n\n\nYes, default: \ntime\n\n\n\n\n\n\n\n\ndiffing two streams\n\n\nfrom flume import *\n\n(\n    (emit(points=[\n        {'time': '2010-01-01T00:00:00.000Z', 'a': 0},\n        {'time': '2010-01-01T00:01:00.000Z', 'a': 1},\n        {'time': '2010-01-01T00:02:00.000Z', 'a': 2}\n    ]),\n    emit(points=[\n        {'time': '2010-01-01T00:01:00.000Z', 'a': 1},\n        {'time': '2010-01-01T00:02:00.000Z', 'a': 2},\n        {'time': '2010-01-01T00:05:00.000Z', 'a': 3}\n    ]))\n    | diff()\n    | write('stdio')\n).execute()\n\n\n\n\nThe above would produce the output:\n\n\n{\na\n: 0, \ntime\n: \n2010-01-01T00:00:00.000Z\n}\n{\na\n: 3, \ntime\n: \n2010-01-01T00:05:00.000Z\n}\n\n\n\n\nThe only points to come out the other side of the \ndiff\n are the \na==0\n and\n\na==3\n point which are not found in both streams.", 
            "title": "diff"
        }, 
        {
            "location": "/procs/diff/#diff", 
            "text": "The  diff  proc is part of the set operators that can be used to join two\nstreams back into one but applying some set operator to the items of the\nstream. In this case only allow points that don't appear in all of the\ninputs.  ( ... , ....) | diff(*fieldnames) | ...     Argument  Description  Required?      *fieldnames  list or tuple of field names to calculate the diff on  Yes, default:  time", 
            "title": "diff"
        }, 
        {
            "location": "/procs/diff/#diffing-two-streams", 
            "text": "from flume import *\n\n(\n    (emit(points=[\n        {'time': '2010-01-01T00:00:00.000Z', 'a': 0},\n        {'time': '2010-01-01T00:01:00.000Z', 'a': 1},\n        {'time': '2010-01-01T00:02:00.000Z', 'a': 2}\n    ]),\n    emit(points=[\n        {'time': '2010-01-01T00:01:00.000Z', 'a': 1},\n        {'time': '2010-01-01T00:02:00.000Z', 'a': 2},\n        {'time': '2010-01-01T00:05:00.000Z', 'a': 3}\n    ]))\n    | diff()\n    | write('stdio')\n).execute()  The above would produce the output:  { a : 0,  time :  2010-01-01T00:00:00.000Z }\n{ a : 3,  time :  2010-01-01T00:05:00.000Z }  The only points to come out the other side of the  diff  are the  a==0  and a==3  point which are not found in both streams.", 
            "title": "diffing two streams"
        }, 
        {
            "location": "/procs/filter/", 
            "text": "filter\n\n\nThe \nfilter\n proc is used to apply \nfilter expression\n\nat any given point in the \nflume\n pipeline.\n\n\n...  | filter(expression) | ...\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nexpression\n\n\nspecifies the filter expression to apply\n\n\nYes\n\n\n\n\n\n\n\n\ndropping odd numbers\n\n\nfrom flume import *\n\n(\n    emit(limit=10)\n    | put(count=count())\n    | filter('count % 2 == 0')\n    | write('stdio')\n).execute()\n\n\n\n\nThe above would produce the output:\n\n\n{\ncount\n: 2, \ntime\n: \n2016-08-01T22:28:12.416Z\n}\n{\ncount\n: 4, \ntime\n: \n2016-08-01T22:28:14.416Z\n}\n{\ncount\n: 6, \ntime\n: \n2016-08-01T22:28:16.416Z\n}\n{\ncount\n: 8, \ntime\n: \n2016-08-01T22:28:18.416Z\n}\n{\ncount\n: 10, \ntime\n: \n2016-08-01T22:28:20.416Z\n}", 
            "title": "filter"
        }, 
        {
            "location": "/procs/filter/#filter", 
            "text": "The  filter  proc is used to apply  filter expression \nat any given point in the  flume  pipeline.  ...  | filter(expression) | ...     Argument  Description  Required?      expression  specifies the filter expression to apply  Yes", 
            "title": "filter"
        }, 
        {
            "location": "/procs/filter/#dropping-odd-numbers", 
            "text": "from flume import *\n\n(\n    emit(limit=10)\n    | put(count=count())\n    | filter('count % 2 == 0')\n    | write('stdio')\n).execute()  The above would produce the output:  { count : 2,  time :  2016-08-01T22:28:12.416Z }\n{ count : 4,  time :  2016-08-01T22:28:14.416Z }\n{ count : 6,  time :  2016-08-01T22:28:16.416Z }\n{ count : 8,  time :  2016-08-01T22:28:18.416Z }\n{ count : 10,  time :  2016-08-01T22:28:20.416Z }", 
            "title": "dropping odd numbers"
        }, 
        {
            "location": "/procs/head/", 
            "text": "head\n\n\nThe \nhead\n proc is used to keep only the first N points in a stream.\n\n\n...  | head(how_many) | ...\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nhow_many\n\n\nnumber of points from the start to let through\n\n\nYes\n\n\n\n\n\n\n\n\nkeep first 5 points\n\n\nfrom flume import *\n\n(\n    emit(limit=10)\n    | put(count=count())\n    | keep('count'\n    | head(5)\n    | write('stdio')\n).execute()\n\n\n\n\nThe above would produce the output:\n\n\n{\ncount\n: 1}\n{\ncount\n: 2}\n{\ncount\n: 3}\n{\ncount\n: 4}\n{\ncount\n: 5}", 
            "title": "head"
        }, 
        {
            "location": "/procs/head/#head", 
            "text": "The  head  proc is used to keep only the first N points in a stream.  ...  | head(how_many) | ...     Argument  Description  Required?      how_many  number of points from the start to let through  Yes", 
            "title": "head"
        }, 
        {
            "location": "/procs/head/#keep-first-5-points", 
            "text": "from flume import *\n\n(\n    emit(limit=10)\n    | put(count=count())\n    | keep('count'\n    | head(5)\n    | write('stdio')\n).execute()  The above would produce the output:  { count : 1}\n{ count : 2}\n{ count : 3}\n{ count : 4}\n{ count : 5}", 
            "title": "keep first 5 points"
        }, 
        {
            "location": "/procs/intersect/", 
            "text": "intersect\n\n\nThe \nintersect\n proc is part of the set operators that can be used to join two\nstreams back into one but applying some set operator to the items of the\nstream. In this case only allow points that appear in both streams to pass.\n\n\n...\n( ... , ....) | intersect(*fieldnames) | ...\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\n*fieldnames\n\n\nlist or tuple of field names to calculate the intersect on\n\n\nNo, default: \ntime\n\n\n\n\n\n\n\n\nintersecting two streams\n\n\nfrom flume import *\n\n(\n    (emit(points=[\n        {'time': '2010-01-01T00:00:00.000Z', 'a': 0},\n        {'time': '2010-01-01T00:01:00.000Z', 'a': 1},\n        {'time': '2010-01-01T00:02:00.000Z', 'a': 2}\n    ]),\n    emit(points=[\n        {'time': '2010-01-01T00:01:00.000Z', 'a': 1},\n        {'time': '2010-01-01T00:02:00.000Z', 'a': 2},\n        {'time': '2010-01-01T00:05:00.000Z', 'a': 3}\n    ]))\n    | intersect()\n    | write('stdio')\n).execute()\n\n\n\n\nThe above would produce the output:\n\n\n{\na\n: 1, \ntime\n: \n2010-01-01T00:01:00.000Z\n}\n{\na\n: 2, \ntime\n: \n2010-01-01T00:02:00.000Z\n}", 
            "title": "intersect"
        }, 
        {
            "location": "/procs/intersect/#intersect", 
            "text": "The  intersect  proc is part of the set operators that can be used to join two\nstreams back into one but applying some set operator to the items of the\nstream. In this case only allow points that appear in both streams to pass.  ...\n( ... , ....) | intersect(*fieldnames) | ...     Argument  Description  Required?      *fieldnames  list or tuple of field names to calculate the intersect on  No, default:  time", 
            "title": "intersect"
        }, 
        {
            "location": "/procs/intersect/#intersecting-two-streams", 
            "text": "from flume import *\n\n(\n    (emit(points=[\n        {'time': '2010-01-01T00:00:00.000Z', 'a': 0},\n        {'time': '2010-01-01T00:01:00.000Z', 'a': 1},\n        {'time': '2010-01-01T00:02:00.000Z', 'a': 2}\n    ]),\n    emit(points=[\n        {'time': '2010-01-01T00:01:00.000Z', 'a': 1},\n        {'time': '2010-01-01T00:02:00.000Z', 'a': 2},\n        {'time': '2010-01-01T00:05:00.000Z', 'a': 3}\n    ]))\n    | intersect()\n    | write('stdio')\n).execute()  The above would produce the output:  { a : 1,  time :  2010-01-01T00:01:00.000Z }\n{ a : 2,  time :  2010-01-01T00:02:00.000Z }", 
            "title": "intersecting two streams"
        }, 
        {
            "location": "/procs/keep/", 
            "text": "keep\n\n\nThe \nkeep\n proc is used to keep only the field names specified from every\npoint in the stream.\n\n\n...  | keep(*fieldnames) | ...\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\n*fieldnames\n\n\nlist of field names to keep in each point\n\n\nNo, default: \nNone\n\n\n\n\n\n\n\n\nkeep just the count field\n\n\nfrom flume import *\n\n(\n    emit(limit=5)\n    | put(count=count())\n    | keep('count'\n    | write('stdio')\n).execute()\n\n\n\n\nThe above would produce the output:\n\n\n{\ncount\n: 1}\n{\ncount\n: 2}\n{\ncount\n: 3}\n{\ncount\n: 4}\n{\ncount\n: 5}", 
            "title": "keep"
        }, 
        {
            "location": "/procs/keep/#keep", 
            "text": "The  keep  proc is used to keep only the field names specified from every\npoint in the stream.  ...  | keep(*fieldnames) | ...     Argument  Description  Required?      *fieldnames  list of field names to keep in each point  No, default:  None", 
            "title": "keep"
        }, 
        {
            "location": "/procs/keep/#keep-just-the-count-field", 
            "text": "from flume import *\n\n(\n    emit(limit=5)\n    | put(count=count())\n    | keep('count'\n    | write('stdio')\n).execute()  The above would produce the output:  { count : 1}\n{ count : 2}\n{ count : 3}\n{ count : 4}\n{ count : 5}", 
            "title": "keep just the count field"
        }, 
        {
            "location": "/procs/put/", 
            "text": "put\n\n\nThe \nput\n proc is used to add fields to point in the current stream.\n\n\n...  | put(*field_assignments) | ...\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\n*field_assignments\n\n\nlist of field assignments to make to each point\n\n\nNo, default: \nNone\n\n\n\n\n\n\n\n\nA field assignment can be as simple as \nfoo='bar'\n or can also be an assignment\nto a \nreducer\n such as \nfoo=count()\n.\n\n\nadd a count field\n\n\nfrom flume import *\n\n(\n    emit(limit=5)\n    | put(count=count())\n    | keep('count'\n    | write('stdio')\n).execute()\n\n\n\n\nThe above would produce the output:\n\n\n{\ncount\n: 1}\n{\ncount\n: 2}\n{\ncount\n: 3}\n{\ncount\n: 4}\n{\ncount\n: 5}", 
            "title": "put"
        }, 
        {
            "location": "/procs/put/#put", 
            "text": "The  put  proc is used to add fields to point in the current stream.  ...  | put(*field_assignments) | ...     Argument  Description  Required?      *field_assignments  list of field assignments to make to each point  No, default:  None     A field assignment can be as simple as  foo='bar'  or can also be an assignment\nto a  reducer  such as  foo=count() .", 
            "title": "put"
        }, 
        {
            "location": "/procs/put/#add-a-count-field", 
            "text": "from flume import *\n\n(\n    emit(limit=5)\n    | put(count=count())\n    | keep('count'\n    | write('stdio')\n).execute()  The above would produce the output:  { count : 1}\n{ count : 2}\n{ count : 3}\n{ count : 4}\n{ count : 5}", 
            "title": "add a count field"
        }, 
        {
            "location": "/procs/reduce/", 
            "text": "reduce\n\n\nThe \nreduce\n proc is used to calculate reductions over the stream of points\nusing \nreducers\n and a few arguments to get the desired calculation\ndone.\n\n\n...  | reduce(every=moment.forever(),\n              reset=True,\n              by=None,\n              *field_assignments) | ...\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nevery\n\n\nthe interval at which to compute the reduction over.\n\n\nNo, default: \nmoment.forever()\n\n\n\n\n\n\nreset\n\n\nspecifies if we should reset the reducers at the end of \nevery\n interval\n\n\nNo, default: \nTrue\n\n\n\n\n\n\nby\n\n\nspecifies the list of fields to calculate the reductions over\n\n\nNo, default: \ntime\n\n\n\n\n\n\n*field_assignments\n\n\nlist of field assignments to make to each point\n\n\nNo, default: \nNone\n\n\n\n\n\n\n\n\nA field assignment can be as simple as \nfoo='bar'\n or can also be an assignment\nto a \nreducer\n such as \nfoo=count()\n.\n\n\ncount days in each month\n\n\nfrom flume import *\n\n(\n    emit(limit=365, start='2015-01-01', every='1d')\n    | reduce(count=count(), every='1 month')\n    | write('stdio')\n).execute()\n\n\n\n\nThe above would produce the output:\n\n\n{\ncount\n: 31, \ntime\n: \n2015-01-01T00:00:00.000Z\n}\n{\ncount\n: 28, \ntime\n: \n2015-02-01T00:00:00.000Z\n}\n{\ncount\n: 31, \ntime\n: \n2015-03-01T00:00:00.000Z\n}\n{\ncount\n: 30, \ntime\n: \n2015-04-01T00:00:00.000Z\n}\n{\ncount\n: 31, \ntime\n: \n2015-05-01T00:00:00.000Z\n}\n{\ncount\n: 30, \ntime\n: \n2015-06-01T00:00:00.000Z\n}\n{\ncount\n: 31, \ntime\n: \n2015-07-01T00:00:00.000Z\n}\n{\ncount\n: 31, \ntime\n: \n2015-08-01T00:00:00.000Z\n}\n{\ncount\n: 30, \ntime\n: \n2015-09-01T00:00:00.000Z\n}\n{\ncount\n: 31, \ntime\n: \n2015-10-01T00:00:00.000Z\n}\n{\ncount\n: 30, \ntime\n: \n2015-11-01T00:00:00.000Z\n}\n{\ncount\n: 31, \ntime\n: \n2015-12-01T00:00:00.000Z\n}", 
            "title": "reduce"
        }, 
        {
            "location": "/procs/reduce/#reduce", 
            "text": "The  reduce  proc is used to calculate reductions over the stream of points\nusing  reducers  and a few arguments to get the desired calculation\ndone.  ...  | reduce(every=moment.forever(),\n              reset=True,\n              by=None,\n              *field_assignments) | ...     Argument  Description  Required?      every  the interval at which to compute the reduction over.  No, default:  moment.forever()    reset  specifies if we should reset the reducers at the end of  every  interval  No, default:  True    by  specifies the list of fields to calculate the reductions over  No, default:  time    *field_assignments  list of field assignments to make to each point  No, default:  None     A field assignment can be as simple as  foo='bar'  or can also be an assignment\nto a  reducer  such as  foo=count() .", 
            "title": "reduce"
        }, 
        {
            "location": "/procs/reduce/#count-days-in-each-month", 
            "text": "from flume import *\n\n(\n    emit(limit=365, start='2015-01-01', every='1d')\n    | reduce(count=count(), every='1 month')\n    | write('stdio')\n).execute()  The above would produce the output:  { count : 31,  time :  2015-01-01T00:00:00.000Z }\n{ count : 28,  time :  2015-02-01T00:00:00.000Z }\n{ count : 31,  time :  2015-03-01T00:00:00.000Z }\n{ count : 30,  time :  2015-04-01T00:00:00.000Z }\n{ count : 31,  time :  2015-05-01T00:00:00.000Z }\n{ count : 30,  time :  2015-06-01T00:00:00.000Z }\n{ count : 31,  time :  2015-07-01T00:00:00.000Z }\n{ count : 31,  time :  2015-08-01T00:00:00.000Z }\n{ count : 30,  time :  2015-09-01T00:00:00.000Z }\n{ count : 31,  time :  2015-10-01T00:00:00.000Z }\n{ count : 30,  time :  2015-11-01T00:00:00.000Z }\n{ count : 31,  time :  2015-12-01T00:00:00.000Z }", 
            "title": "count days in each month"
        }, 
        {
            "location": "/procs/remove/", 
            "text": "remove\n\n\nThe \nremove\n proc is used to remove only the field names specified from every\npoint in the stream.\n\n\n...  | remove(*fieldnames) | ...\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\n*fieldnames\n\n\nlist of field names to remove in each point\n\n\nNo, default: \nNone\n\n\n\n\n\n\n\n\nremove just the time field\n\n\nfrom flume import *\n\n(\n    emit(limit=5)\n    | put(count=count())\n    | remove('time')\n    | write('stdio')\n).execute()\n\n\n\n\nThe above would produce the output:\n\n\n{\ncount\n: 1}\n{\ncount\n: 2}\n{\ncount\n: 3}\n{\ncount\n: 4}\n{\ncount\n: 5}", 
            "title": "remove"
        }, 
        {
            "location": "/procs/remove/#remove", 
            "text": "The  remove  proc is used to remove only the field names specified from every\npoint in the stream.  ...  | remove(*fieldnames) | ...     Argument  Description  Required?      *fieldnames  list of field names to remove in each point  No, default:  None", 
            "title": "remove"
        }, 
        {
            "location": "/procs/remove/#remove-just-the-time-field", 
            "text": "from flume import *\n\n(\n    emit(limit=5)\n    | put(count=count())\n    | remove('time')\n    | write('stdio')\n).execute()  The above would produce the output:  { count : 1}\n{ count : 2}\n{ count : 3}\n{ count : 4}\n{ count : 5}", 
            "title": "remove just the time field"
        }, 
        {
            "location": "/procs/seq/", 
            "text": "seq\n\n\nThe \nseq\n proc is used to serialize the execution of N \nflume\n pipelines so\nthat they execute in the same order that you've passed them to the \nseq\n proc.\n\n\n...  | seq(*pipelines) | ...\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\n*pipelines\n\n\nlist of \nflume\n pipelines to execute serially\n\n\nYes\n\n\n\n\n\n\n\n\nserialize the execution of two emits\n\n\nfrom flume import *\n\n(\n    seq(emit(limit=5, every='1s'),\n        emit(limit=5, every='0.5s'))\n    | write('stdio')\n).execute()\n\n\n\n\nThe above would produce the output:\n\n\n{\ntime\n: \n2016-08-12T22:03:06.156Z\n}\n{\ntime\n: \n2016-08-12T22:03:07.156Z\n}\n{\ntime\n: \n2016-08-12T22:03:08.156Z\n}\n{\ntime\n: \n2016-08-12T22:03:09.156Z\n}\n{\ntime\n: \n2016-08-12T22:03:10.156Z\n}\n{\ntime\n: \n2016-08-12T22:03:11.167Z\n}\n{\ntime\n: \n2016-08-12T22:03:11.667Z\n}\n{\ntime\n: \n2016-08-12T22:03:12.167Z\n}\n{\ntime\n: \n2016-08-12T22:03:12.667Z\n}\n{\ntime\n: \n2016-08-12T22:03:13.167Z\n}\n\n\n\n\nAnd you'll see how your stream will emit 1 point per second for 5s and then\nanother 5 points in half that time. Which is a small example of how the \nseq\n\nproc can be useful.", 
            "title": "seq"
        }, 
        {
            "location": "/procs/seq/#seq", 
            "text": "The  seq  proc is used to serialize the execution of N  flume  pipelines so\nthat they execute in the same order that you've passed them to the  seq  proc.  ...  | seq(*pipelines) | ...     Argument  Description  Required?      *pipelines  list of  flume  pipelines to execute serially  Yes", 
            "title": "seq"
        }, 
        {
            "location": "/procs/seq/#serialize-the-execution-of-two-emits", 
            "text": "from flume import *\n\n(\n    seq(emit(limit=5, every='1s'),\n        emit(limit=5, every='0.5s'))\n    | write('stdio')\n).execute()  The above would produce the output:  { time :  2016-08-12T22:03:06.156Z }\n{ time :  2016-08-12T22:03:07.156Z }\n{ time :  2016-08-12T22:03:08.156Z }\n{ time :  2016-08-12T22:03:09.156Z }\n{ time :  2016-08-12T22:03:10.156Z }\n{ time :  2016-08-12T22:03:11.167Z }\n{ time :  2016-08-12T22:03:11.667Z }\n{ time :  2016-08-12T22:03:12.167Z }\n{ time :  2016-08-12T22:03:12.667Z }\n{ time :  2016-08-12T22:03:13.167Z }  And you'll see how your stream will emit 1 point per second for 5s and then\nanother 5 points in half that time. Which is a small example of how the  seq \nproc can be useful.", 
            "title": "serialize the execution of two emits"
        }, 
        {
            "location": "/procs/sort/", 
            "text": "sort\n\n\nThe \nsort\n proc is used to sort your points by a specific field (other than\ntime). When you use \nsort\n the resulting points have no \ntime\n field\nassociated.\n\n\n...  | sort(*fieldnames,\n            order='asc',\n            limit=100000) | ...\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\n*fieldnames\n\n\nlist or tuple of field names to sort the stream by\n\n\nNo, default: \ntime\n\n\n\n\n\n\norder\n\n\nasc\n to order ascending and \ndesc\n to order points by descending order\n\n\nNo, default: \nasc\n\n\n\n\n\n\nlimit\n\n\nbuffering limit set to avoid running out of memory\n\n\nNo, default: \n100000\n\n\n\n\n\n\n\n\nsort stream descending by count\n\n\nfrom flume import *\n\n(\n    emit(limit=5)\n    | put(count=count())\n    | sort('count', order='desc')\n    | write('stdio')\n).execute()\n\n\n\n\nThe above would produce the output:\n\n\n{\ncount\n: 5}\n{\ncount\n: 4}\n{\ncount\n: 3}\n{\ncount\n: 2}\n{\ncount\n: 1}", 
            "title": "sort"
        }, 
        {
            "location": "/procs/sort/#sort", 
            "text": "The  sort  proc is used to sort your points by a specific field (other than\ntime). When you use  sort  the resulting points have no  time  field\nassociated.  ...  | sort(*fieldnames,\n            order='asc',\n            limit=100000) | ...     Argument  Description  Required?      *fieldnames  list or tuple of field names to sort the stream by  No, default:  time    order  asc  to order ascending and  desc  to order points by descending order  No, default:  asc    limit  buffering limit set to avoid running out of memory  No, default:  100000", 
            "title": "sort"
        }, 
        {
            "location": "/procs/sort/#sort-stream-descending-by-count", 
            "text": "from flume import *\n\n(\n    emit(limit=5)\n    | put(count=count())\n    | sort('count', order='desc')\n    | write('stdio')\n).execute()  The above would produce the output:  { count : 5}\n{ count : 4}\n{ count : 3}\n{ count : 2}\n{ count : 1}", 
            "title": "sort stream descending by count"
        }, 
        {
            "location": "/procs/tail/", 
            "text": "tail\n\n\nThe \ntail\n proc is used to keep only the last N points in a stream.\n\n\n...  | tail(how_many) | ...\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nhow_many\n\n\nnumber of points from the end to let through\n\n\nYes\n\n\n\n\n\n\n\n\nkeep last 5 points\n\n\nfrom flume import *\n\n(\n    emit(limit=10)\n    | put(count=count())\n    | keep('count'\n    | tail(5)\n    | write('stdio')\n).execute()\n\n\n\n\nThe above would produce the output:\n\n\n{\ncount\n: 6}\n{\ncount\n: 7}\n{\ncount\n: 8}\n{\ncount\n: 9}\n{\ncount\n: 10}", 
            "title": "tail"
        }, 
        {
            "location": "/procs/tail/#tail", 
            "text": "The  tail  proc is used to keep only the last N points in a stream.  ...  | tail(how_many) | ...     Argument  Description  Required?      how_many  number of points from the end to let through  Yes", 
            "title": "tail"
        }, 
        {
            "location": "/procs/tail/#keep-last-5-points", 
            "text": "from flume import *\n\n(\n    emit(limit=10)\n    | put(count=count())\n    | keep('count'\n    | tail(5)\n    | write('stdio')\n).execute()  The above would produce the output:  { count : 6}\n{ count : 7}\n{ count : 8}\n{ count : 9}\n{ count : 10}", 
            "title": "keep last 5 points"
        }, 
        {
            "location": "/procs/union/", 
            "text": "union\n\n\nThe \nunion\n proc is part of the set operators that can be used to join two\nstreams back into one but applying some set operator to the items of the\nstream. In this case it allows points from both streams to pass and\nde-duplicates common points.\n\n\n...\n( ... , ....) | union(*fieldnames) | ...\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\n*fieldnames\n\n\nlist or tuple of field names to calculate the union on\n\n\nNo, default: \ntime\n\n\n\n\n\n\n\n\nunioning two streams\n\n\nfrom flume import *\n\n(\n    (emit(points=[\n        {'time': '2010-01-01T00:00:00.000Z', 'a': 0},\n        {'time': '2010-01-01T00:01:00.000Z', 'a': 1},\n        {'time': '2010-01-01T00:02:00.000Z', 'a': 2}\n    ]),\n    emit(points=[\n        {'time': '2010-01-01T00:01:00.000Z', 'a': 1},\n        {'time': '2010-01-01T00:02:00.000Z', 'a': 2},\n        {'time': '2010-01-01T00:05:00.000Z', 'a': 3}\n    ]))\n    | union()\n    | write('stdio')\n).execute()\n\n\n\n\nThe above would produce the output:\n\n\n{\na\n: 0, \ntime\n: \n2010-01-01T00:00:00.000Z\n}\n{\na\n: 1, \ntime\n: \n2010-01-01T00:01:00.000Z\n}\n{\na\n: 2, \ntime\n: \n2010-01-01T00:02:00.000Z\n}\n{\na\n: 3, \ntime\n: \n2010-01-01T00:05:00.000Z\n}", 
            "title": "union"
        }, 
        {
            "location": "/procs/union/#union", 
            "text": "The  union  proc is part of the set operators that can be used to join two\nstreams back into one but applying some set operator to the items of the\nstream. In this case it allows points from both streams to pass and\nde-duplicates common points.  ...\n( ... , ....) | union(*fieldnames) | ...     Argument  Description  Required?      *fieldnames  list or tuple of field names to calculate the union on  No, default:  time", 
            "title": "union"
        }, 
        {
            "location": "/procs/union/#unioning-two-streams", 
            "text": "from flume import *\n\n(\n    (emit(points=[\n        {'time': '2010-01-01T00:00:00.000Z', 'a': 0},\n        {'time': '2010-01-01T00:01:00.000Z', 'a': 1},\n        {'time': '2010-01-01T00:02:00.000Z', 'a': 2}\n    ]),\n    emit(points=[\n        {'time': '2010-01-01T00:01:00.000Z', 'a': 1},\n        {'time': '2010-01-01T00:02:00.000Z', 'a': 2},\n        {'time': '2010-01-01T00:05:00.000Z', 'a': 3}\n    ]))\n    | union()\n    | write('stdio')\n).execute()  The above would produce the output:  { a : 0,  time :  2010-01-01T00:00:00.000Z }\n{ a : 1,  time :  2010-01-01T00:01:00.000Z }\n{ a : 2,  time :  2010-01-01T00:02:00.000Z }\n{ a : 3,  time :  2010-01-01T00:05:00.000Z }", 
            "title": "unioning two streams"
        }, 
        {
            "location": "/procs/reduce/reducers/", 
            "text": "reducers\n\n\nreducers are responsible for handling a small piece of computing a value over a\nset of points. The way this works is that the reducer will receive individual\nupdate calls with each point that makes it to the \nreduce\n\n/\nput\n.\n\n\nbuilt-in\n\n\n\n\ncount\n \n\n\ndate\n \n\n\nfuncr\n\n\niterate\n \n\n\nmath\n \n\n\nmaximum\n \n\n\nminimum\n \n\n\n\n\nwrite your own\n\n\nWriting your own reducer is an easy task and should be a common one for various\nreducers not yet part of the core \nflume\n project. To write one you simply\nextend from the \nflume.reducer\n class and implement the following methods:\n\n\n\n\n__init__(self, ...)\n make sure to use the constructor to handle passing\n                            in the field names from the points that you will \n                            be manipulating.\n\n\n\n\nupdate(self, point)\n this is called for every point that makes it into\n                            the current interval of reduction. You should use\n                            the names of the fields you stored previously in \n                            the constructor to compute the value you wanted.\n\n\n\n\n\n\nresult(self)\n should return the current value of the reduction being\n                     calculated. This means that if you're just counting \n                     the appearance of a field you'd return the current count.\n\n\n\n\n\n\nreset(self)\n reset is called at the end of \nevery\n interval as defined\n                    in the call to the \nreduce\n proc.", 
            "title": "overview"
        }, 
        {
            "location": "/procs/reduce/reducers/#reducers", 
            "text": "reducers are responsible for handling a small piece of computing a value over a\nset of points. The way this works is that the reducer will receive individual\nupdate calls with each point that makes it to the  reduce \n/ put .", 
            "title": "reducers"
        }, 
        {
            "location": "/procs/reduce/reducers/#built-in", 
            "text": "count    date    funcr  iterate    math    maximum    minimum", 
            "title": "built-in"
        }, 
        {
            "location": "/procs/reduce/reducers/#write-your-own", 
            "text": "Writing your own reducer is an easy task and should be a common one for various\nreducers not yet part of the core  flume  project. To write one you simply\nextend from the  flume.reducer  class and implement the following methods:   __init__(self, ...)  make sure to use the constructor to handle passing\n                            in the field names from the points that you will \n                            be manipulating.   update(self, point)  this is called for every point that makes it into\n                            the current interval of reduction. You should use\n                            the names of the fields you stored previously in \n                            the constructor to compute the value you wanted.    result(self)  should return the current value of the reduction being\n                     calculated. This means that if you're just counting \n                     the appearance of a field you'd return the current count.    reset(self)  reset is called at the end of  every  interval as defined\n                    in the call to the  reduce  proc.", 
            "title": "write your own"
        }, 
        {
            "location": "/procs/reduce/reducers/count/", 
            "text": "count\n\n\nThe \ncount\n reducer simply counts the number of points that have passed\nthrough this point in the pipeline.\n\n\ncounting points\n\n\nfrom flume import *\n\n(\n    emit(limit=10, start='2015-01-01')\n    | reduce(count=count())\n    | write('stdio')\n).execute()\n\n\n\n\nThe above would produce the output:\n\n\n{\ncount\n: 10, \ntime\n: \n2015-01-01T00:00:00.000Z\n}", 
            "title": "count"
        }, 
        {
            "location": "/procs/reduce/reducers/count/#count", 
            "text": "The  count  reducer simply counts the number of points that have passed\nthrough this point in the pipeline.", 
            "title": "count"
        }, 
        {
            "location": "/procs/reduce/reducers/count/#counting-points", 
            "text": "from flume import *\n\n(\n    emit(limit=10, start='2015-01-01')\n    | reduce(count=count())\n    | write('stdio')\n).execute()  The above would produce the output:  { count : 10,  time :  2015-01-01T00:00:00.000Z }", 
            "title": "counting points"
        }, 
        {
            "location": "/procs/reduce/reducers/date/", 
            "text": "date\n\n\nThe \ndate\n reducers can be used to easily extract information from existing\ndatetime fields such as the \ntime\n field. You'll see below how to use each one.\n\n\n.year\n\n\n... | reduce(year=date.year('time')) | ...\n\n\n\n\nConverts a \ndatetime\n field value into the string that represents the year, such\nas \n2016\n, \n1975\n, etc. Can be used both in the \nput\n and\n\nreduce\n processors.\n\n\n.fullmonth\n\n\n... | reduce(fullmonth=date.fullmonth('time')) | ...\n\n\n\n\nConverts a \ndatetime\n field value into the string that represents the full\nmonth, such as \nJanuary\n, \nDecember\n, etc. Can be used both in the \nput\n and\n\nreduce\n processors.\n\n\n.month\n\n\n... | reduce(month=date.month('time')) | ...\n\n\n\n\nConverts a \ndatetime\n field value into the string that represents the abreviated\nmonth, such as \nJan\n, \nDec\n, etc. Can be used both in the \nput\n and\n\nreduce\n processors.\n\n\n.fullweekday\n\n\n... | reduce(fullweekday=date.fullweekday('time')) | ...\n\n\n\n\nConverts a \ndatetime\n field value into the string that represents the full\nweekday, such as \nTuesday\n, \nSunday\n, etc. Can be used both in the\n\nput\n and \nreduce\n processors.\n\n\n.weekday\n\n\n... | reduce(weekday=date.weekday('time')) | ...\n\n\n\n\nConverts a \ndatetime\n field value into the string that represents the abreviated\nweekday, such as \nTue\n, \nSun\n, etc. Can be used both in the \nput\n\nand \nreduce\n processors.\n\n\n.strftime\n\n\n... | reduce(shortdate=date.strftime('time', '%Y/%m/%d')) | ...\n\n\n\n\nConverts a \ndatetime\n field value into the format specified using python's\n\nstrftime\n format from \nhere\n \nCan be used both in the \nput\n and \nreduce\n processors.", 
            "title": "date"
        }, 
        {
            "location": "/procs/reduce/reducers/date/#date", 
            "text": "The  date  reducers can be used to easily extract information from existing\ndatetime fields such as the  time  field. You'll see below how to use each one.", 
            "title": "date"
        }, 
        {
            "location": "/procs/reduce/reducers/date/#year", 
            "text": "... | reduce(year=date.year('time')) | ...  Converts a  datetime  field value into the string that represents the year, such\nas  2016 ,  1975 , etc. Can be used both in the  put  and reduce  processors.", 
            "title": ".year"
        }, 
        {
            "location": "/procs/reduce/reducers/date/#fullmonth", 
            "text": "... | reduce(fullmonth=date.fullmonth('time')) | ...  Converts a  datetime  field value into the string that represents the full\nmonth, such as  January ,  December , etc. Can be used both in the  put  and reduce  processors.", 
            "title": ".fullmonth"
        }, 
        {
            "location": "/procs/reduce/reducers/date/#month", 
            "text": "... | reduce(month=date.month('time')) | ...  Converts a  datetime  field value into the string that represents the abreviated\nmonth, such as  Jan ,  Dec , etc. Can be used both in the  put  and reduce  processors.", 
            "title": ".month"
        }, 
        {
            "location": "/procs/reduce/reducers/date/#fullweekday", 
            "text": "... | reduce(fullweekday=date.fullweekday('time')) | ...  Converts a  datetime  field value into the string that represents the full\nweekday, such as  Tuesday ,  Sunday , etc. Can be used both in the put  and  reduce  processors.", 
            "title": ".fullweekday"
        }, 
        {
            "location": "/procs/reduce/reducers/date/#weekday", 
            "text": "... | reduce(weekday=date.weekday('time')) | ...  Converts a  datetime  field value into the string that represents the abreviated\nweekday, such as  Tue ,  Sun , etc. Can be used both in the  put \nand  reduce  processors.", 
            "title": ".weekday"
        }, 
        {
            "location": "/procs/reduce/reducers/date/#strftime", 
            "text": "... | reduce(shortdate=date.strftime('time', '%Y/%m/%d')) | ...  Converts a  datetime  field value into the format specified using python's strftime  format from  here  \nCan be used both in the  put  and  reduce  processors.", 
            "title": ".strftime"
        }, 
        {
            "location": "/procs/reduce/reducers/funcr/", 
            "text": "funcr\n\n\nThe \nfuncr\n reducer allows you to apply any operation to a field in the\n\nflume\n stream by creating a reducer dynamically.\n\n\n... | [put|reduce](value=funcr(function)(fieldname)) | ...\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nfunction\n\n\nfunction name or lambda expression\n\n\nYes\n\n\n\n\n\n\nfieldname\n\n\nfieldname to apply the function to at streaming time\n\n\nYes\n\n\n\n\n\n\n\n\ncount by increments of 0.1\n\n\nfrom flume import *\n\n(\n    emit(limit=10, start='2015-01-01')\n    | put(count=count())\n    | put(count=funcr(lambda value: value / 10.0)('count'))\n    | write('stdio')\n).execute()\n\n\n\n\nThe above would produce the output:\n\n\n{\ncount\n: 0.1, \ntime\n: \n2015-01-01T00:00:00.000Z\n}\n{\ncount\n: 0.2, \ntime\n: \n2015-01-01T00:00:01.000Z\n}\n{\ncount\n: 0.3, \ntime\n: \n2015-01-01T00:00:02.000Z\n}\n{\ncount\n: 0.4, \ntime\n: \n2015-01-01T00:00:03.000Z\n}\n{\ncount\n: 0.5, \ntime\n: \n2015-01-01T00:00:04.000Z\n}\n{\ncount\n: 0.6, \ntime\n: \n2015-01-01T00:00:05.000Z\n}\n{\ncount\n: 0.7, \ntime\n: \n2015-01-01T00:00:06.000Z\n}\n{\ncount\n: 0.8, \ntime\n: \n2015-01-01T00:00:07.000Z\n}\n{\ncount\n: 0.9, \ntime\n: \n2015-01-01T00:00:08.000Z\n}\n{\ncount\n: 1.0, \ntime\n: \n2015-01-01T00:00:09.000Z\n}", 
            "title": "funcr"
        }, 
        {
            "location": "/procs/reduce/reducers/funcr/#funcr", 
            "text": "The  funcr  reducer allows you to apply any operation to a field in the flume  stream by creating a reducer dynamically.  ... | [put|reduce](value=funcr(function)(fieldname)) | ...     Argument  Description  Required?      function  function name or lambda expression  Yes    fieldname  fieldname to apply the function to at streaming time  Yes", 
            "title": "funcr"
        }, 
        {
            "location": "/procs/reduce/reducers/funcr/#count-by-increments-of-01", 
            "text": "from flume import *\n\n(\n    emit(limit=10, start='2015-01-01')\n    | put(count=count())\n    | put(count=funcr(lambda value: value / 10.0)('count'))\n    | write('stdio')\n).execute()  The above would produce the output:  { count : 0.1,  time :  2015-01-01T00:00:00.000Z }\n{ count : 0.2,  time :  2015-01-01T00:00:01.000Z }\n{ count : 0.3,  time :  2015-01-01T00:00:02.000Z }\n{ count : 0.4,  time :  2015-01-01T00:00:03.000Z }\n{ count : 0.5,  time :  2015-01-01T00:00:04.000Z }\n{ count : 0.6,  time :  2015-01-01T00:00:05.000Z }\n{ count : 0.7,  time :  2015-01-01T00:00:06.000Z }\n{ count : 0.8,  time :  2015-01-01T00:00:07.000Z }\n{ count : 0.9,  time :  2015-01-01T00:00:08.000Z }\n{ count : 1.0,  time :  2015-01-01T00:00:09.000Z }", 
            "title": "count by increments of 0.1"
        }, 
        {
            "location": "/procs/reduce/reducers/iterate/", 
            "text": "iterate\n\n\nThe \niterate\n reducer will iterate through the values in the list it was given\nand cycle through those as to \"decorate\" your data with values from that list.\n\n\nlabeling points\n\n\nfrom flume import *\n\n(\n    emit(limit=10, start='2015-01-01')\n    | put(label=iterate(['a','b','c']))\n    | write('stdio')\n).execute()\n\n\n\n\nThe above would produce the output:\n\n\n{\ntime\n: \n2015-01-01T00:00:00.000Z\n, \nlabel\n: \na\n}\n{\ntime\n: \n2015-01-01T00:00:01.000Z\n, \nlabel\n: \nb\n}\n{\ntime\n: \n2015-01-01T00:00:02.000Z\n, \nlabel\n: \nc\n}\n{\ntime\n: \n2015-01-01T00:00:03.000Z\n, \nlabel\n: \na\n}\n{\ntime\n: \n2015-01-01T00:00:04.000Z\n, \nlabel\n: \nb\n}\n{\ntime\n: \n2015-01-01T00:00:05.000Z\n, \nlabel\n: \nc\n}\n{\ntime\n: \n2015-01-01T00:00:06.000Z\n, \nlabel\n: \na\n}\n{\ntime\n: \n2015-01-01T00:00:07.000Z\n, \nlabel\n: \nb\n}\n{\ntime\n: \n2015-01-01T00:00:08.000Z\n, \nlabel\n: \nc\n}\n{\ntime\n: \n2015-01-01T00:00:09.000Z\n, \nlabel\n: \na\n}", 
            "title": "iterate"
        }, 
        {
            "location": "/procs/reduce/reducers/iterate/#iterate", 
            "text": "The  iterate  reducer will iterate through the values in the list it was given\nand cycle through those as to \"decorate\" your data with values from that list.", 
            "title": "iterate"
        }, 
        {
            "location": "/procs/reduce/reducers/iterate/#labeling-points", 
            "text": "from flume import *\n\n(\n    emit(limit=10, start='2015-01-01')\n    | put(label=iterate(['a','b','c']))\n    | write('stdio')\n).execute()  The above would produce the output:  { time :  2015-01-01T00:00:00.000Z ,  label :  a }\n{ time :  2015-01-01T00:00:01.000Z ,  label :  b }\n{ time :  2015-01-01T00:00:02.000Z ,  label :  c }\n{ time :  2015-01-01T00:00:03.000Z ,  label :  a }\n{ time :  2015-01-01T00:00:04.000Z ,  label :  b }\n{ time :  2015-01-01T00:00:05.000Z ,  label :  c }\n{ time :  2015-01-01T00:00:06.000Z ,  label :  a }\n{ time :  2015-01-01T00:00:07.000Z ,  label :  b }\n{ time :  2015-01-01T00:00:08.000Z ,  label :  c }\n{ time :  2015-01-01T00:00:09.000Z ,  label :  a }", 
            "title": "labeling points"
        }, 
        {
            "location": "/procs/reduce/reducers/math/", 
            "text": "math\n\n\nThe \nmath\n reducers exposed are the following set of functions from the math \nmodule in python that can be applied against any of the fields in your \nflume\n\nstream. Mostly we exposed the python \nmath\n module from\n\nhere\n.\n\n\nnumber-theoretic and representation functions\n\n\n\n\nmath.ceil(fieldname)\n\n\nmath.fabs(fieldname)\n\n\nmath.floor(fieldname)\n\n\nmath.factorial(fieldname)\n\n\nmath.fmod(fieldname)\n\n\nmath.trunc(fieldname)\n\n\nmath.isinf(fieldname)\n\n\nmath.isnan(fieldname)\n\n\n\n\npower and logarithmic functions\n\n\n\n\nmath.exp(fieldname)\n\n\nmath.expm1(fieldname)\n\n\nmath.log(fieldname)\n\n\nmath.log1p(fieldname)\n\n\nmath.log10(fieldname)\n\n\nmath.pow(fieldname, power)\n\n\nmath.sqrt(fieldname)\n\n\n\n\ntrigonometric functions\n\n\n\n\nmath.acos(fieldname)\n\n\nmath.asin(fieldname)\n\n\nmath.atan(fieldname)\n\n\nmath.atan2(fieldname)\n\n\nmath.cos(fieldname)\n\n\nmath.sin(fieldname)\n\n\nmath.tan(fieldname)\n\n\n\n\nangular conversion\n\n\n\n\nmath.degrees(fieldname)\n\n\nmath.radians(fieldname)", 
            "title": "math"
        }, 
        {
            "location": "/procs/reduce/reducers/math/#math", 
            "text": "The  math  reducers exposed are the following set of functions from the math \nmodule in python that can be applied against any of the fields in your  flume \nstream. Mostly we exposed the python  math  module from here .", 
            "title": "math"
        }, 
        {
            "location": "/procs/reduce/reducers/math/#number-theoretic-and-representation-functions", 
            "text": "math.ceil(fieldname)  math.fabs(fieldname)  math.floor(fieldname)  math.factorial(fieldname)  math.fmod(fieldname)  math.trunc(fieldname)  math.isinf(fieldname)  math.isnan(fieldname)", 
            "title": "number-theoretic and representation functions"
        }, 
        {
            "location": "/procs/reduce/reducers/math/#power-and-logarithmic-functions", 
            "text": "math.exp(fieldname)  math.expm1(fieldname)  math.log(fieldname)  math.log1p(fieldname)  math.log10(fieldname)  math.pow(fieldname, power)  math.sqrt(fieldname)", 
            "title": "power and logarithmic functions"
        }, 
        {
            "location": "/procs/reduce/reducers/math/#trigonometric-functions", 
            "text": "math.acos(fieldname)  math.asin(fieldname)  math.atan(fieldname)  math.atan2(fieldname)  math.cos(fieldname)  math.sin(fieldname)  math.tan(fieldname)", 
            "title": "trigonometric functions"
        }, 
        {
            "location": "/procs/reduce/reducers/math/#angular-conversion", 
            "text": "math.degrees(fieldname)  math.radians(fieldname)", 
            "title": "angular conversion"
        }, 
        {
            "location": "/procs/reduce/reducers/maximum/", 
            "text": "maximum\n\n\nThe \nmaximum\n reducer picks the maximum value of the field provided during the\nreduction interval.\n\n\nmaximum value for a field\n\n\nfrom flume import *\n\n(\n    emit(limit=10, start='2015-01-01')\n    | put(count=count())\n    | reduce(maximum=maximum('count'))\n    | write('stdio')\n).execute()\n\n\n\n\nThe above would produce the output:\n\n\n{\nmaximum\n: 10, \ntime\n: \n2015-01-01T00:00:00.000Z\n}", 
            "title": "maximum"
        }, 
        {
            "location": "/procs/reduce/reducers/maximum/#maximum", 
            "text": "The  maximum  reducer picks the maximum value of the field provided during the\nreduction interval.", 
            "title": "maximum"
        }, 
        {
            "location": "/procs/reduce/reducers/maximum/#maximum-value-for-a-field", 
            "text": "from flume import *\n\n(\n    emit(limit=10, start='2015-01-01')\n    | put(count=count())\n    | reduce(maximum=maximum('count'))\n    | write('stdio')\n).execute()  The above would produce the output:  { maximum : 10,  time :  2015-01-01T00:00:00.000Z }", 
            "title": "maximum value for a field"
        }, 
        {
            "location": "/procs/reduce/reducers/minimum/", 
            "text": "minimum\n\n\nThe \nminimum\n reducer picks the maximum value of the field provided during the\nreduction interval.\n\n\nminimum value for a field\n\n\nfrom flume import *\n\n(\n    emit(limit=10, start='2015-01-01')\n    | put(count=count())\n    | reduce(minimum=maximum('count'))\n    | write('stdio')\n).execute()\n\n\n\n\nThe above would produce the output:\n\n\n{\nminimum\n: 10, \ntime\n: \n2015-01-01T00:00:00.000Z\n}", 
            "title": "minimum"
        }, 
        {
            "location": "/procs/reduce/reducers/minimum/#minimum", 
            "text": "The  minimum  reducer picks the maximum value of the field provided during the\nreduction interval.", 
            "title": "minimum"
        }, 
        {
            "location": "/procs/reduce/reducers/minimum/#minimum-value-for-a-field", 
            "text": "from flume import *\n\n(\n    emit(limit=10, start='2015-01-01')\n    | put(count=count())\n    | reduce(minimum=maximum('count'))\n    | write('stdio')\n).execute()  The above would produce the output:  { minimum : 10,  time :  2015-01-01T00:00:00.000Z }", 
            "title": "minimum value for a field"
        }, 
        {
            "location": "/sinks/views/", 
            "text": "views\n\n\nViews are an abstraction that allow us to have different implementations for\nvisualizing our \nbarchart\n, \nlinechart\n, etc. sinks\ndepending on the current need. Currently supported views:\n\n\n\n\ngnuplot\n\n\npygal\n\n\n\n\nwrite your own\n\n\nYou can quite easily write your own \nview\n implementation by extending from\nthe \nflume.sinks.views.base.base\n class and then implementing the following\nmethods:\n\n\n\n\nrender(self, chart_type, data)\n where the chart_type can currently be\n    one of \nbarchart\n, \nlinechart\n or \ntimechart\n and the data is a dictionary\n    that maps a series name to the (x,y) pairs that represent it.\n\n\nlinechart - the series names match to lines on the chart where the x,y\n              values are the pairs in the list.\n\n\ntimechart - the series names match to lines on the chart where the x,y\n              values are the pairs in the list and x is a datetime object.\n\n\nbarchart - the series names are the names of the set of bars that have a\n             height of y for the x (may or may not be a datetime object)\n             multiple series represent multiple bars for the same value of x.\n\n\n\n\n\n\n\n\nFor an example checkout the source of the\n\ngnuplot\n\nor \npygal\n\nviews.\n\n\nThe render method may be called many times with intermediate results that\ncontain the data up to this point and the view implementation must deal with\nthis how it sees fit such as re-rendering the whole output or simply redrawing\nthe data that changed.", 
            "title": "overview"
        }, 
        {
            "location": "/sinks/views/#views", 
            "text": "Views are an abstraction that allow us to have different implementations for\nvisualizing our  barchart ,  linechart , etc. sinks\ndepending on the current need. Currently supported views:   gnuplot  pygal", 
            "title": "views"
        }, 
        {
            "location": "/sinks/views/#write-your-own", 
            "text": "You can quite easily write your own  view  implementation by extending from\nthe  flume.sinks.views.base.base  class and then implementing the following\nmethods:   render(self, chart_type, data)  where the chart_type can currently be\n    one of  barchart ,  linechart  or  timechart  and the data is a dictionary\n    that maps a series name to the (x,y) pairs that represent it.  linechart - the series names match to lines on the chart where the x,y\n              values are the pairs in the list.  timechart - the series names match to lines on the chart where the x,y\n              values are the pairs in the list and x is a datetime object.  barchart - the series names are the names of the set of bars that have a\n             height of y for the x (may or may not be a datetime object)\n             multiple series represent multiple bars for the same value of x.     For an example checkout the source of the gnuplot \nor  pygal \nviews.  The render method may be called many times with intermediate results that\ncontain the data up to this point and the view implementation must deal with\nthis how it sees fit such as re-rendering the whole output or simply redrawing\nthe data that changed.", 
            "title": "write your own"
        }, 
        {
            "location": "/sinks/views/gnuplot/", 
            "text": "gnuplot\n\n\nThe \ngnuplot\n view is a view implementation that can render the various views\nthat \nflume\n supports (ie barchart, linechart and timechart) using\n\ngnuplot\n which has the ability to render those\ncharts on the console using ASCII art or to an X11 window on Linux which is\nsomewhat interactive.\n\n\n...\n| chart_type('gnuplot',\n             title='',\n             width=1280,\n             height=1024,\n             terminal='dumb')\n\n\n\n\nwhere chart_type can be \nbarchart\n, \nlinechart\n,\n\ntimechart\n.\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\ntitle\n\n\ntitle to use on the resulting chart\n\n\nYes, default: \nNone\n\n\n\n\n\n\nwidth\n\n\nwidth of the chart to render in pixels\n\n\nNo, default: \n1280\n\n\n\n\n\n\nheight\n\n\nheight of the chart to render in pixels\n\n\nNo, default: \n1024\n\n\n\n\n\n\nterminal\n\n\ngnuplot \nterminal\n type to use\n\n\nNo, default: \nterminal\n\n\n\n\n\n\ntimefmt\n\n\nexposes the \ntimefmt\n from gnuplot\n\n\nNo, default: \n%Y/%d/%m-%H:%M:%S", 
            "title": "gnuplot"
        }, 
        {
            "location": "/sinks/views/gnuplot/#gnuplot", 
            "text": "The  gnuplot  view is a view implementation that can render the various views\nthat  flume  supports (ie barchart, linechart and timechart) using gnuplot  which has the ability to render those\ncharts on the console using ASCII art or to an X11 window on Linux which is\nsomewhat interactive.  ...\n| chart_type('gnuplot',\n             title='',\n             width=1280,\n             height=1024,\n             terminal='dumb')  where chart_type can be  barchart ,  linechart , timechart .     Argument  Description  Required?      title  title to use on the resulting chart  Yes, default:  None    width  width of the chart to render in pixels  No, default:  1280    height  height of the chart to render in pixels  No, default:  1024    terminal  gnuplot  terminal  type to use  No, default:  terminal    timefmt  exposes the  timefmt  from gnuplot  No, default:  %Y/%d/%m-%H:%M:%S", 
            "title": "gnuplot"
        }, 
        {
            "location": "/sinks/views/pygal/", 
            "text": "pygal\n\n\nThe \npygal\n view is a view implementation that can render the various views\nthat \nflume\n supports (ie barchart, linechart and timechart) using\n\npygal\n which has the ability to render those\ncharts on a file or even in a \njupyter-notebook\n\n\n...\n| chart_type('pygal',\n             title='',\n             width=1280,\n             height=1024,\n             filename=None,\n             format='png')\n\n\n\n\nwhere chart_type can be \nbarchart\n, \nlinechart\n,\n\ntimechart\n.\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\ntitle\n\n\ntitle to use on the resulting chart\n\n\nYes, default: \nNone\n\n\n\n\n\n\nwidth\n\n\nwidth of the chart to render in pixels\n\n\nNo, default: \n1280\n\n\n\n\n\n\nheight\n\n\nheight of the chart to render in pixels\n\n\nNo, default: \n1024\n\n\n\n\n\n\nfilename\n\n\nfilename to write the output image to (*)\n\n\nNo, default: \nNone\n\n\n\n\n\n\nformat\n\n\noutput formats supported: \npng\n and \nhtml\n. (*)\n\n\nNo, default \npng\n\n\n\n\n\n\n\n\n(*) optional when in a jupyter-notebook session.", 
            "title": "pygal"
        }, 
        {
            "location": "/sinks/views/pygal/#pygal", 
            "text": "The  pygal  view is a view implementation that can render the various views\nthat  flume  supports (ie barchart, linechart and timechart) using pygal  which has the ability to render those\ncharts on a file or even in a  jupyter-notebook  ...\n| chart_type('pygal',\n             title='',\n             width=1280,\n             height=1024,\n             filename=None,\n             format='png')  where chart_type can be  barchart ,  linechart , timechart .     Argument  Description  Required?      title  title to use on the resulting chart  Yes, default:  None    width  width of the chart to render in pixels  No, default:  1280    height  height of the chart to render in pixels  No, default:  1024    filename  filename to write the output image to (*)  No, default:  None    format  output formats supported:  png  and  html . (*)  No, default  png     (*) optional when in a jupyter-notebook session.", 
            "title": "pygal"
        }, 
        {
            "location": "/sinks/", 
            "text": "sinks\n\n\nA sink is the end point of any \nflume\n pipeline and with a sink you can write\nor visualize the output of your \nflume\n stream.\n\n\nbuilt-in\n\n\n\n\nbarchart\n\n\nlinechart\n\n\nmemory\n\n\ntimechart\n\n\nwrite\n\n\n\n\nwrite your own\n\n\ncoming soon...", 
            "title": "overview"
        }, 
        {
            "location": "/sinks/#sinks", 
            "text": "A sink is the end point of any  flume  pipeline and with a sink you can write\nor visualize the output of your  flume  stream.", 
            "title": "sinks"
        }, 
        {
            "location": "/sinks/#built-in", 
            "text": "barchart  linechart  memory  timechart  write", 
            "title": "built-in"
        }, 
        {
            "location": "/sinks/#write-your-own", 
            "text": "coming soon...", 
            "title": "write your own"
        }, 
        {
            "location": "/sinks/barchart/", 
            "text": "barchart\n\n\nThe \nbarchart\n sink is will attempt to render a barchart using an underlying\n\nview\n which can be one of a few different view implementations. The\nreason for having the \nbarchart\n sink is to abstract some of the stream \nmanipulation code into single place and have the \nviews\n handle the\nrendering of the already processed data.\n\n\n...\n| barchart(view,\n           series='name',\n           category='time',\n           value='value',\n           **kwargs)\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nview\n\n\nname of the \nview\n to render the barchart through\n\n\nYes, default: \nNone\n\n\n\n\n\n\nseries\n\n\nfield name of the individual series (ie bars on the chart)\n\n\nNo, default: \nname\n\n\n\n\n\n\ncategory\n\n\nfield name of the values the x-axis represents\n\n\nNo, default: \ntime\n\n\n\n\n\n\nvalue\n\n\nfield name of the values the y-axis represents (ie bar height)\n\n\nNo, default: \nvalue\n\n\n\n\n\n\n**kwargs\n\n\nadditional keyword arguments passed to the underlying view\n\n\nNo, default: \nNone", 
            "title": "barchart"
        }, 
        {
            "location": "/sinks/barchart/#barchart", 
            "text": "The  barchart  sink is will attempt to render a barchart using an underlying view  which can be one of a few different view implementations. The\nreason for having the  barchart  sink is to abstract some of the stream \nmanipulation code into single place and have the  views  handle the\nrendering of the already processed data.  ...\n| barchart(view,\n           series='name',\n           category='time',\n           value='value',\n           **kwargs)     Argument  Description  Required?      view  name of the  view  to render the barchart through  Yes, default:  None    series  field name of the individual series (ie bars on the chart)  No, default:  name    category  field name of the values the x-axis represents  No, default:  time    value  field name of the values the y-axis represents (ie bar height)  No, default:  value    **kwargs  additional keyword arguments passed to the underlying view  No, default:  None", 
            "title": "barchart"
        }, 
        {
            "location": "/sinks/linechart/", 
            "text": "linehart\n\n\nThe \nlinehart\n sink is will attempt to render a line chart using an underlying\n\nview\n which can be one of a few different view implementations. The\nreason for having the \nlinechart\n sink is to abstract some of the stream \nmanipulation code into single place and have the \nviews\n handle the\nrendering of the already processed data.\n\n\n...\n| linechart(view,\n            series='name',\n            xvalue='time',\n            yvalue='value',\n            **kwargs)\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nview\n\n\nname of the \nview\n to render the linechart through\n\n\nYes, default: \nNone\n\n\n\n\n\n\nseries\n\n\nfield name of the individual series (ie lines on the chart)\n\n\nNo, default: \nname\n\n\n\n\n\n\nxvalue\n\n\nfield name of the values the x-axis represents\n\n\nNo, default: \ntime\n\n\n\n\n\n\nyvalue\n\n\nfield name of the values the y-axis represents\n\n\nNo, default: \nvalue\n\n\n\n\n\n\n**kwargs\n\n\nadditional keyword arguments passed to the underlying view\n\n\nNo, default: \nNone", 
            "title": "linechart"
        }, 
        {
            "location": "/sinks/linechart/#linehart", 
            "text": "The  linehart  sink is will attempt to render a line chart using an underlying view  which can be one of a few different view implementations. The\nreason for having the  linechart  sink is to abstract some of the stream \nmanipulation code into single place and have the  views  handle the\nrendering of the already processed data.  ...\n| linechart(view,\n            series='name',\n            xvalue='time',\n            yvalue='value',\n            **kwargs)     Argument  Description  Required?      view  name of the  view  to render the linechart through  Yes, default:  None    series  field name of the individual series (ie lines on the chart)  No, default:  name    xvalue  field name of the values the x-axis represents  No, default:  time    yvalue  field name of the values the y-axis represents  No, default:  value    **kwargs  additional keyword arguments passed to the underlying view  No, default:  None", 
            "title": "linehart"
        }, 
        {
            "location": "/sinks/memory/", 
            "text": "memory\n\n\nThe \nmeory\n sink is used primarily for testing but it does allow you to store\nthe points that arrive at the sink in an array.\n\n\n...\n| memory(results)\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nresults\n\n\na \nPython\n list where all of the points will be appended to\n\n\nYes, default: \nNone", 
            "title": "memory"
        }, 
        {
            "location": "/sinks/memory/#memory", 
            "text": "The  meory  sink is used primarily for testing but it does allow you to store\nthe points that arrive at the sink in an array.  ...\n| memory(results)     Argument  Description  Required?      results  a  Python  list where all of the points will be appended to  Yes, default:  None", 
            "title": "memory"
        }, 
        {
            "location": "/sinks/timechart/", 
            "text": "timechart\n\n\nThe \ntimehart\n sink is will attempt to render a time chart using an underlying\n\nview\n which can be one of a few different view implementations. The\nreason for having the \ntimechart\n sink is to abstract some of the stream \nmanipulation code into single place and have the \nviews\n handle the\nrendering of the already processed data.\n\n\n...\n| timechart(view,\n            series='name',\n            yvalue='value',\n            **kwargs)\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nview\n\n\nname of the \nview\n to render the timechart through\n\n\nYes, default: \nNone\n\n\n\n\n\n\nseries\n\n\nfield name of the individual series (ie lines on the chart)\n\n\nNo, default: \nname\n\n\n\n\n\n\nyvalue\n\n\nfield name of the values the y-axis represents\n\n\nNo, default: \nvalue\n\n\n\n\n\n\n**kwargs\n\n\nadditional keyword arguments passed to the underlying view\n\n\nNo, default: \nNone", 
            "title": "timechart"
        }, 
        {
            "location": "/sinks/timechart/#timechart", 
            "text": "The  timehart  sink is will attempt to render a time chart using an underlying view  which can be one of a few different view implementations. The\nreason for having the  timechart  sink is to abstract some of the stream \nmanipulation code into single place and have the  views  handle the\nrendering of the already processed data.  ...\n| timechart(view,\n            series='name',\n            yvalue='value',\n            **kwargs)     Argument  Description  Required?      view  name of the  view  to render the timechart through  Yes, default:  None    series  field name of the individual series (ie lines on the chart)  No, default:  name    yvalue  field name of the values the y-axis represents  No, default:  value    **kwargs  additional keyword arguments passed to the underlying view  No, default:  None", 
            "title": "timechart"
        }, 
        {
            "location": "/sinks/write/", 
            "text": "write\n\n\nThe \nwrite\n sink is responsible for handling points between \nflume\n and\nan \nadapter\n. The main reason this model is that the \nwrite\n\nsource can handle some of the common tasks that adapters would otherwise\nhave to implement themselves.\n\n\n...\n| write(adapter,\n        batch=-1)\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nadapter\n\n\nname of the \nadapter\n to write to\n\n\nYes, default: \nNone\n\n\n\n\n\n\nbatch\n\n\nbatch size to use when handing points to the adapter, default is to send as we receive\n\n\nNo, default: \n-1", 
            "title": "write"
        }, 
        {
            "location": "/sinks/write/#write", 
            "text": "The  write  sink is responsible for handling points between  flume  and\nan  adapter . The main reason this model is that the  write \nsource can handle some of the common tasks that adapters would otherwise\nhave to implement themselves.  ...\n| write(adapter,\n        batch=-1)     Argument  Description  Required?      adapter  name of the  adapter  to write to  Yes, default:  None    batch  batch size to use when handing points to the adapter, default is to send as we receive  No, default:  -1", 
            "title": "write"
        }, 
        {
            "location": "/sources/", 
            "text": "sources\n\n\nA source is the starting point of any \nflume\n pipeline and with a source\nyou can read points from just about any source of data as long as there is an\n\nadapter\n for it.\n\n\nbuilt-in\n\n\n\n\nemit\n\n\nread\n\n\n\n\nwrite your own\n\n\ncoming soon...", 
            "title": "overview"
        }, 
        {
            "location": "/sources/#sources", 
            "text": "A source is the starting point of any  flume  pipeline and with a source\nyou can read points from just about any source of data as long as there is an adapter  for it.", 
            "title": "sources"
        }, 
        {
            "location": "/sources/#built-in", 
            "text": "emit  read", 
            "title": "built-in"
        }, 
        {
            "location": "/sources/#write-your-own", 
            "text": "coming soon...", 
            "title": "write your own"
        }, 
        {
            "location": "/sources/emit/", 
            "text": "emit\n\n\nThe \nemit\n source is primarily used for testing and can be used to generate\npoints from a specific point in time at a specific interval.\n\n\nemit(limit=-1,\n     every='1s',\n     points=None,\n     start=None,\n     end=moment.end()) | ...\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nlimit\n\n\ntotal number of points to emit, when not set emits forever\n\n\nNo, default: \n-1\n\n\n\n\n\n\nevery\n\n\nmoments.duration\n specifying the rate at which to emit points\n\n\nNo, default: \n1s\n\n\n\n\n\n\npoints\n\n\na list of points to emit in order\n\n\nNo, default: \nNone\n\n\n\n\n\n\nstart\n\n\nmoments.date\n specifying the exact date in time to start\n\n\nNo, default: \nNone\n\n\n\n\n\n\nend\n\n\nmoments.date\n specifying the exact date in time to stop emitting\n\n\nNo, default: \nmoment.end()\n\n\n\n\n\n\n\n\nemitting a point for every day in 2013\n\n\nfrom flume import emit, write\n\n(\n    emit(limit=365, start='2013-01-01', every='1 day')\n    | write('stdio')\n).execute()", 
            "title": "emit"
        }, 
        {
            "location": "/sources/emit/#emit", 
            "text": "The  emit  source is primarily used for testing and can be used to generate\npoints from a specific point in time at a specific interval.  emit(limit=-1,\n     every='1s',\n     points=None,\n     start=None,\n     end=moment.end()) | ...     Argument  Description  Required?      limit  total number of points to emit, when not set emits forever  No, default:  -1    every  moments.duration  specifying the rate at which to emit points  No, default:  1s    points  a list of points to emit in order  No, default:  None    start  moments.date  specifying the exact date in time to start  No, default:  None    end  moments.date  specifying the exact date in time to stop emitting  No, default:  moment.end()", 
            "title": "emit"
        }, 
        {
            "location": "/sources/emit/#emitting-a-point-for-every-day-in-2013", 
            "text": "from flume import emit, write\n\n(\n    emit(limit=365, start='2013-01-01', every='1 day')\n    | write('stdio')\n).execute()", 
            "title": "emitting a point for every day in 2013"
        }, 
        {
            "location": "/sources/read/", 
            "text": "read\n\n\nThe \nread\n source is responsible for handling points between\n\nadapters\n and the rest of the \nflume\n pipeline. The main\nreason this model is that the \nread\n source can handle some of the common\ntasks that adapters would otherwise have to implement themselves.\n\n\nread(adapter,\n     time='time') | ...\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nadapter\n\n\nthe name of the \nadapter\n to read from\n\n\nYes, default: \nNone\n\n\n\n\n\n\ntime\n\n\nthe name of the field that contains the \ntime\n value\n\n\nNo, default: \ntime", 
            "title": "read"
        }, 
        {
            "location": "/sources/read/#read", 
            "text": "The  read  source is responsible for handling points between adapters  and the rest of the  flume  pipeline. The main\nreason this model is that the  read  source can handle some of the common\ntasks that adapters would otherwise have to implement themselves.  read(adapter,\n     time='time') | ...     Argument  Description  Required?      adapter  the name of the  adapter  to read from  Yes, default:  None    time  the name of the field that contains the  time  value  No, default:  time", 
            "title": "read"
        }, 
        {
            "location": "/thirdparty/", 
            "text": "third party\n\n\nThe third party package holds a few helpers to make using \nflume\n much easier\nby packaging up things into easy to use \nsources\n,\n\nsinks\n, \nreducers\n, etc.", 
            "title": "overview"
        }, 
        {
            "location": "/thirdparty/#third-party", 
            "text": "The third party package holds a few helpers to make using  flume  much easier\nby packaging up things into easy to use  sources , sinks ,  reducers , etc.", 
            "title": "third party"
        }, 
        {
            "location": "/thirdparty/github/", 
            "text": "github\n\n\nA collection of github utilities that allow you to easily read data from github.\n\n\nissues api\n\n\nuser issues source\n\n\ngithub.issues.user(oauth=None,\n                   **parameters)\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\noauth\n\n\nthe github oauth token to use when reading the authenticated users issues\n\n\nYes\n\n\n\n\n\n\nparameters\n\n\nparameters\n\n\n\n\n\n\n\n\n\n\nThe \nparameters\n are any of the parameters exposed by the API that we've\nexposed as a keyword argument.\n\n\norgs issues source\n\n\ngithub.issues.orgs(org_name,\n                   oauth=None,\n                   **parametrs)\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\norg_name\n\n\nname of the organization on github\n\n\nYes\n\n\n\n\n\n\noauth\n\n\nthe github oauth token to use when reading the authenticated users issues\n\n\nYes\n\n\n\n\n\n\nparameters\n\n\nparameters\n\n\n\n\n\n\n\n\n\n\nThe \nparameters\n are any of the parameters exposed by the API that we've\nexposed as a keyword argument.\n\n\nrepo issues source\n\n\ngithub.issues.repo(owner,\n                   repo_name,\n                   oauth=None,\n                   **parametrs)\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nowner\n\n\nname of the repo owner on github\n\n\nYes\n\n\n\n\n\n\nrepo_name\n\n\nname of the repo name on github\n\n\nYes\n\n\n\n\n\n\noauth\n\n\nthe github oauth token to use when reading the authenticated users issues\n\n\nYes\n\n\n\n\n\n\nparameters\n\n\nparameters\n\n\n\n\n\n\n\n\n\n\nThe \nparameters\n are any of the parameters exposed by the API that we've\nexposed as a keyword argument.\n\n\nissue source\n\n\ngithub.issues.issue(owner,\n                    repo_name,\n                    issue_number,\n                    oauth=None)\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nowner\n\n\nname of the repo owner on github\n\n\nYes\n\n\n\n\n\n\nrepo_name\n\n\nname of the repo name on github\n\n\nYes\n\n\n\n\n\n\nissue_nubmer\n\n\nthe github issue number to retrieve the details for\n\n\nYes\n\n\n\n\n\n\noauth\n\n\nthe github oauth token to use when reading the authenticated users issues\n\n\nYes\n\n\n\n\n\n\n\n\npulls api\n\n\nrepo pull requests source\n\n\ngithub.pulls.repo(owner,\n                  repo_name,\n                  oauth=None,\n                  **parameters)\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nowner\n\n\nname of the repo owner on github\n\n\nYes\n\n\n\n\n\n\nrepo_name\n\n\nname of the repo name on github\n\n\nYes\n\n\n\n\n\n\noauth\n\n\nthe github oauth token to use when reading the authenticated users issues\n\n\nYes\n\n\n\n\n\n\nparameters\n\n\nparameters\n\n\nNo\n\n\n\n\n\n\n\n\nThe \nparameters\n are any of the parameters exposed by the API that we've\nexposed as a keyword argument.\n\n\npull request commits source\n\n\ngithub.pulls.commits(owner,\n                     repo_name,\n                     pull_number,\n                     oauth=None,\n                     **parameters)\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nowner\n\n\nname of the repo owner on github\n\n\nYes\n\n\n\n\n\n\nrepo_name\n\n\nname of the repo name on github\n\n\nYes\n\n\n\n\n\n\npull_number\n\n\npull number to get the commits associated with from github\n\n\nYes\n\n\n\n\n\n\noauth\n\n\nthe github oauth token to use when reading the authenticated users issues\n\n\nYes", 
            "title": "github"
        }, 
        {
            "location": "/thirdparty/github/#github", 
            "text": "A collection of github utilities that allow you to easily read data from github.", 
            "title": "github"
        }, 
        {
            "location": "/thirdparty/github/#issues-api", 
            "text": "", 
            "title": "issues api"
        }, 
        {
            "location": "/thirdparty/github/#user-issues-source", 
            "text": "github.issues.user(oauth=None,\n                   **parameters)     Argument  Description  Required?      oauth  the github oauth token to use when reading the authenticated users issues  Yes    parameters  parameters      The  parameters  are any of the parameters exposed by the API that we've\nexposed as a keyword argument.", 
            "title": "user issues source"
        }, 
        {
            "location": "/thirdparty/github/#orgs-issues-source", 
            "text": "github.issues.orgs(org_name,\n                   oauth=None,\n                   **parametrs)     Argument  Description  Required?      org_name  name of the organization on github  Yes    oauth  the github oauth token to use when reading the authenticated users issues  Yes    parameters  parameters      The  parameters  are any of the parameters exposed by the API that we've\nexposed as a keyword argument.", 
            "title": "orgs issues source"
        }, 
        {
            "location": "/thirdparty/github/#repo-issues-source", 
            "text": "github.issues.repo(owner,\n                   repo_name,\n                   oauth=None,\n                   **parametrs)     Argument  Description  Required?      owner  name of the repo owner on github  Yes    repo_name  name of the repo name on github  Yes    oauth  the github oauth token to use when reading the authenticated users issues  Yes    parameters  parameters      The  parameters  are any of the parameters exposed by the API that we've\nexposed as a keyword argument.", 
            "title": "repo issues source"
        }, 
        {
            "location": "/thirdparty/github/#issue-source", 
            "text": "github.issues.issue(owner,\n                    repo_name,\n                    issue_number,\n                    oauth=None)     Argument  Description  Required?      owner  name of the repo owner on github  Yes    repo_name  name of the repo name on github  Yes    issue_nubmer  the github issue number to retrieve the details for  Yes    oauth  the github oauth token to use when reading the authenticated users issues  Yes", 
            "title": "issue source"
        }, 
        {
            "location": "/thirdparty/github/#pulls-api", 
            "text": "", 
            "title": "pulls api"
        }, 
        {
            "location": "/thirdparty/github/#repo-pull-requests-source", 
            "text": "github.pulls.repo(owner,\n                  repo_name,\n                  oauth=None,\n                  **parameters)     Argument  Description  Required?      owner  name of the repo owner on github  Yes    repo_name  name of the repo name on github  Yes    oauth  the github oauth token to use when reading the authenticated users issues  Yes    parameters  parameters  No     The  parameters  are any of the parameters exposed by the API that we've\nexposed as a keyword argument.", 
            "title": "repo pull requests source"
        }, 
        {
            "location": "/thirdparty/github/#pull-request-commits-source", 
            "text": "github.pulls.commits(owner,\n                     repo_name,\n                     pull_number,\n                     oauth=None,\n                     **parameters)     Argument  Description  Required?      owner  name of the repo owner on github  Yes    repo_name  name of the repo name on github  Yes    pull_number  pull number to get the commits associated with from github  Yes    oauth  the github oauth token to use when reading the authenticated users issues  Yes", 
            "title": "pull request commits source"
        }, 
        {
            "location": "/thirdparty/syslog/", 
            "text": "syslog\n\n\nA collection of syslog utilities that make reading from syslog a whole lot\neasier.\n\n\nsyslog source\n\n\nsyslog(logfile=None,\n       path='/var/log/') | ...\n\n\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\nRequired?\n\n\n\n\n\n\n\n\n\n\nlogfile\n\n\nthe name of the exact syslog file to read (ie /var/log/syslog.7.gz\n\n\nNo, default: \nNone\n\n\n\n\n\n\npath\n\n\nthe path where the syslog files are located (including gzipped ones)\n\n\nNo, default: \n/var/log/\n\n\n\n\n\n\n\n\nWhen you don't set the \nlogfile\n we end up figuring out what syslog files are\nthere and reading through all in order and also handling compression along the\nway for the older syslog files that were rotated and compressed.\n\n\ncalculate syslog lines per day\n\n\nflume \nsyslog() | reduce(count=count(), every='1 day') | write('stdio')\n\n\n\n\n\nexample output:\n\n\n flume \nsyslog() | reduce(count=count(), every='1 day') | write('stdio')\n\n{\ncount\n: 1440, \ntime\n: \n2016-08-08T08:50:46.000Z\n}\n{\ncount\n: 2133, \ntime\n: \n2016-08-09T08:50:46.000Z\n}\n{\ncount\n: 1166, \ntime\n: \n2016-08-10T08:50:46.000Z\n}\n{\ncount\n: 2034, \ntime\n: \n2016-08-11T08:50:46.000Z\n}\n{\ncount\n: 2197, \ntime\n: \n2016-08-12T08:50:46.000Z\n}\n{\ncount\n: 1409, \ntime\n: \n2016-08-13T08:50:46.000Z\n}\n{\ncount\n: 644, \ntime\n: \n2016-08-14T08:50:46.000Z\n}\n{\ncount\n: 2763, \ntime\n: \n2016-08-15T08:50:46.000Z\n}\n\n\n\n\nOr on a \nbarchart\n using \ngnuplot\n:\n\n\nflume \nsyslog() | reduce(value=count(), every='1 day', name='syslog lines', time=date.strftime('time', '%Y-%m-%d')) | barchart('gnuplot')\n\n\n\n\n\nWhich produces for the syslog data on my system:\n\n\n     +----+----------+-----------+----------+-----------+----------+--------+-----------+-----+\n     |    +          +           +          +           +          +        +   syslog lines  |\n     |                                                                                        |\n3000 ++                                                                                      ++\n     |                                                                               ******   |\n2500 ++                                                                              *    *  ++\n     |                                                                               *    *   |\n     |             ******                            ******                          *    *   |\n2000 ++            *    *                 ******     *    *                          *    *  ++\n     |             *    *                 *    *     *    *                          *    *   |\n     |             *    *                 *    *     *    *                          *    *   |\n1500 ++ ******     *    *                 *    *     *    *     ******               *    *  ++\n     |  *    *     *    *                 *    *     *    *     *    *               *    *   |\n     |  *    *     *    *      ******     *    *     *    *     *    *               *    *   |\n1000 ++ *    *     *    *      *    *     *    *     *    *     *    *               *    *  ++\n     |  *    *     *    *      *    *     *    *     *    *     *    *     ******    *    *   |\n 500 ++ *    *     *    *      *    *     *    *     *    *     *    *     *    *    *    *  ++\n     |  *    *     *    *      *    *     *    *     *    *     *    *     *    *    *    *   |\n     |  * +  *     * +  *      * +  *     * +  *     *  + *     *  + *     *  + *    *  + *   |\n   0 ++-******-----******------******-----******-----******-----******-----******----******--++\n       2016-08-08 2016-08-10 2016-08-11 2016-08-12 2016-08-13 2016-08-14 2016-08-14 2016-08-15", 
            "title": "syslog"
        }, 
        {
            "location": "/thirdparty/syslog/#syslog", 
            "text": "A collection of syslog utilities that make reading from syslog a whole lot\neasier.", 
            "title": "syslog"
        }, 
        {
            "location": "/thirdparty/syslog/#syslog-source", 
            "text": "syslog(logfile=None,\n       path='/var/log/') | ...     Argument  Description  Required?      logfile  the name of the exact syslog file to read (ie /var/log/syslog.7.gz  No, default:  None    path  the path where the syslog files are located (including gzipped ones)  No, default:  /var/log/     When you don't set the  logfile  we end up figuring out what syslog files are\nthere and reading through all in order and also handling compression along the\nway for the older syslog files that were rotated and compressed.", 
            "title": "syslog source"
        }, 
        {
            "location": "/thirdparty/syslog/#calculate-syslog-lines-per-day", 
            "text": "flume  syslog() | reduce(count=count(), every='1 day') | write('stdio')   example output:   flume  syslog() | reduce(count=count(), every='1 day') | write('stdio') \n{ count : 1440,  time :  2016-08-08T08:50:46.000Z }\n{ count : 2133,  time :  2016-08-09T08:50:46.000Z }\n{ count : 1166,  time :  2016-08-10T08:50:46.000Z }\n{ count : 2034,  time :  2016-08-11T08:50:46.000Z }\n{ count : 2197,  time :  2016-08-12T08:50:46.000Z }\n{ count : 1409,  time :  2016-08-13T08:50:46.000Z }\n{ count : 644,  time :  2016-08-14T08:50:46.000Z }\n{ count : 2763,  time :  2016-08-15T08:50:46.000Z }  Or on a  barchart  using  gnuplot :  flume  syslog() | reduce(value=count(), every='1 day', name='syslog lines', time=date.strftime('time', '%Y-%m-%d')) | barchart('gnuplot')   Which produces for the syslog data on my system:       +----+----------+-----------+----------+-----------+----------+--------+-----------+-----+\n     |    +          +           +          +           +          +        +   syslog lines  |\n     |                                                                                        |\n3000 ++                                                                                      ++\n     |                                                                               ******   |\n2500 ++                                                                              *    *  ++\n     |                                                                               *    *   |\n     |             ******                            ******                          *    *   |\n2000 ++            *    *                 ******     *    *                          *    *  ++\n     |             *    *                 *    *     *    *                          *    *   |\n     |             *    *                 *    *     *    *                          *    *   |\n1500 ++ ******     *    *                 *    *     *    *     ******               *    *  ++\n     |  *    *     *    *                 *    *     *    *     *    *               *    *   |\n     |  *    *     *    *      ******     *    *     *    *     *    *               *    *   |\n1000 ++ *    *     *    *      *    *     *    *     *    *     *    *               *    *  ++\n     |  *    *     *    *      *    *     *    *     *    *     *    *     ******    *    *   |\n 500 ++ *    *     *    *      *    *     *    *     *    *     *    *     *    *    *    *  ++\n     |  *    *     *    *      *    *     *    *     *    *     *    *     *    *    *    *   |\n     |  * +  *     * +  *      * +  *     * +  *     *  + *     *  + *     *  + *    *  + *   |\n   0 ++-******-----******------******-----******-----******-----******-----******----******--++\n       2016-08-08 2016-08-10 2016-08-11 2016-08-12 2016-08-13 2016-08-14 2016-08-14 2016-08-15", 
            "title": "calculate syslog lines per day"
        }
    ]
}